\documentclass{beamer}

\usepackage{mathtools}
\usepackage[ruled]{algorithm2e}
\usepackage[authoryear,round]{natbib}

\newcommand*\diff{\mathop{}\!\mathrm{d}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usetheme{Madrid}

\title[Gradient-Free Optimal Postprocessing]{Gradient-Free Optimal Postprocessing \\ of MCMC Output}
\author{Artem Glebov}
\institute{King's College London}
\date{2024}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Overview}

 \begin{block}{Problem}
	Develop a computationally efficient algorithm for summarising the output of a Markov Chain Monte Carlo simulation.
 \end{block}
 
 \vskip -0.5\bigskipamount
 
 \begin{block}{Motivation}
	Uncertainty quantification in a multi-stage simulation of the functioning of the human heart.
 \end{block}
 
 \vskip -0.5\bigskipamount
 
 \begin{block}{Existing solution}
	The optimisation algorithm of \cite{riabizOptimalThinningMCMC2022} to select a subsample of MCMC output that minimises a measure of proximity to the target distribution (kernel Stein discrepancy), which requires the gradients of the log-posterior and is thus expensive.
 \end{block}
 
 \vskip -0.5\bigskipamount
 
 \begin{block}{Proposal}
	Modify the algorithm of \cite{riabizOptimalThinningMCMC2022} to use the gradient-free kernel Stein discrepancy of \cite{fisherGradientFreeKernelStein2024}.
 \end{block}

\end{frame}

\section{Background}

\subsection{Markov Chain Monte Carlo (MCMC)}

\begin{frame}
\frametitle{Markov Chain Monte Carlo}
Markov chain Monte Carlo (MCMC) are a popular class of algorithms for sampling from complex probability distributions.

Given a target distribution $P$ defined on a state space $\mathcal{X}$, an MCMC algorithm proceeds by constructing a chain of random variables $(X_i)_{i=0}^\infty$  which satisfy the Markov property:
\begin{equation*}
\mathbb{P}(X_{i+1}\in A | X_0, \dots, X_i) = \mathbb{P}(X_{i+1}\in A | X_i) \quad\text{for any measurable } A \in \mathcal{X}.
\end{equation*}
Viewed as a function, the right-hand side above is called the Markov transition kernel and is denoted 
\begin{equation*}
R(A | x) \coloneq \mathbb{P}(X_{i+1}\in A | X_i = x).
\end{equation*}
The transition kernel $R$ is selected so that it is easy to sample from and to ensure asymptotic convergence to the target distribution $P$:
\begin{equation*}
P_i \xrightarrow[]{d} P \quad\text{as}\quad i \to \infty.
\end{equation*}
A sample of size $n$ is a realisation $(x_i)_{i=0}^n$ of the first $n$ variables in the chain, which is constructed sequentially.
\end{frame}

\subsection{Challenges of running MCMC}

\begin{frame}
\frametitle{Challenges of running MCMC}

\begin{enumerate}
\item The choice of a starting point for a chain.
\item Exploring the modes of a multimodal distribution.
\item Calibrating the scale of the proposal distribution.
\item Convergence detection.
\item Detecting and eliminating the burn-in.
\item Autocorrelation between samples in a chain.
\item Compressing sample for further expensive processing.
\end{enumerate}

\end{frame}

\subsection{Stein thinning}

\begin{frame}
\frametitle{Thinning}

\begin{block}{Problem}
Given MCMC output $(x_i)_{i=1}^n$ of length $n$, identify $m \ll n$ indices $\pi(j) \in \{1,\dots, n\}$ with $j\in\{1, \dots, m\}$, such that the approximation provided by the subset of samples
\begin{equation*}
\frac{1}{m} \sum_{j=1}^m \delta(x_{\pi(j)})
\label{eq:thinned-sample}
\end{equation*}
is closest to the target distribution.
\end{block}

We need a measure of proximity of the selected subsample to the target distribution.

\end{frame}

\begin{frame}
\frametitle{Measure of proximity}

\begin{block}{Integral probability metric}
An integral probability metric between two distributions $P$ and $P'$ is defined as
\begin{equation*}
\mathcal{D}_{\mathcal{F}}(P, P') \coloneq \sup_{f \in \mathcal{F}}\left|\int_\mathcal{X} f \diff P - \int_\mathcal{X} f \diff P' \right|,
\label{eq:ipm}
\end{equation*}
where $\mathcal{X}$ is a measurable space on which both $P$ and $P'$ are defined and $\mathcal{F}$ is a set of test functions.
\end{block}

\only<1>{
The metric is said to be \textit{measure-determining} if
\begin{equation*}
\mathcal{D}_{\mathcal{F}}(P, P') = 0 \quad \text{iff} \quad P = P',
\end{equation*}
and it offers \textit{convergence control} if 
\begin{equation*}
\mathcal{D}_{\mathcal{F}}(P, P'_m) \to 0 \quad \text{implies} \quad P'_m \xrightarrow[]{d} P
\end{equation*}
as $m \to \infty$, for any sequence of distributions $P'_m$.
}
\only<2>{
However, it is \alert{difficult to compute} in practice:
\begin{itemize}
\item the integral $\int_\mathcal{X} f \diff P$ is often intractable,
\item the supremum requires optimisation.
\end{itemize}
}

\end{frame}

\begin{frame}
\frametitle{Stein discrepancy}

\begin{block}{Integral probability metric}
An integral probability metric between two distributions $P$ and $P'$ is defined as
\begin{equation*}
\mathcal{D}_{\mathcal{F}}(P, P') \coloneq \sup_{f \in \mathcal{F}}\left|\int_\mathcal{X} f \diff P - \int_\mathcal{X} f \diff P' \right|,
\label{eq:ipm}
\end{equation*}
where $\mathcal{X}$ is a measurable space on which both $P$ and $P'$ are defined and $\mathcal{F}$ is a set of test functions.
\end{block}

\begin{block}{Idea}

Avoid the need to evaluate $\int_\mathcal{X} f \diff P$ by choosing a set of functions $\mathcal{F}$ such that $\int_\mathcal{X} f \diff P = 0$ for all $f \in \mathcal{F}$.

\end{block}

\end{frame}

\begin{frame}
\frametitle{Stein discrepancy (continued)}

\cite{gorhamMeasuringSampleQuality2015} observed that the infinitesimal generator of a Markov process $(Z_t)_{t \geq 0}$ given by
\begin{equation*}
(\mathcal{L}u)(x) \coloneq \lim_{t \to 0} \frac{\mathbb{E}[u(Z_t) | Z_0 = x] - u(x)}{t} \quad \text{for } u:\mathbb{R}^d \to \mathbb{R}
\end{equation*}
satisfies
\begin{equation*}
\mathbb{E}[(\mathcal{L} u) (Z)] = 0
\end{equation*}
under mild conditions on $\mathcal{L}$ and $u$.

In the specific case of an overdamped Langevin diffusion
$$\diff Z_t = \frac{1}{2} \nabla \log p(Z_t) \diff t + \diff W_t,$$
where $p$ is the density of $P$ and $W_t$ is the standard Brownian motion, the infinitesimal generator becomes
$$(\mathcal{L}_P u)(x) = \frac{1}{2} \langle \nabla u(x), \nabla \log p(x)\rangle + \frac{1}{2}\langle \nabla, \nabla u(x) \rangle.$$

\end{frame}

\begin{frame}
\frametitle{Stein discrepancy (continued)}

The infinitesimal generator of an overdamped Langevin diffusion:
$$(\mathcal{L}_P u)(x) = \frac{1}{2} \langle \nabla u(x), \nabla \log p(x)\rangle + \frac{1}{2}\langle \nabla, \nabla u(x) \rangle.$$

Denoting $g  = \frac{1}{2}\nabla u$, \cite{gorhamMeasuringSampleQuality2015} obtain the Stein operator
\begin{equation*}
\mathcal{A}_P g \coloneq \langle g, \nabla \log p \rangle + \langle \nabla, g \rangle = \langle p^{-1}\nabla, p g \rangle,
\label{eq:stein-operator}
\end{equation*}
and rewrite the expression for the integral probability metric as
\begin{equation*}
\mathcal{D}_{P, \mathcal{G}}(P') = \sup_{g \in \mathcal{G}}\left|\int_\mathcal{X} \mathcal{A}_P g \diff P' \right|
\label{eq:stein-discrepancy-g}
\end{equation*}
for a suitably chosen set $\mathcal{G}$.

\end{frame}

\begin{frame}
\frametitle{Stein discrepancy (continued)}

Using the Langevin Stein operator, the integral probability metric specialises to
\begin{block}{Stein discrepancy}
\begin{equation*}
\mathcal{D}_{P, \mathcal{G}}(P') = \sup_{g \in \mathcal{G}}\left|\int_\mathcal{X} \mathcal{A}_P g \diff P' \right|
\end{equation*}
\end{block}

The difficulty evaluating the supremum still remains.

\begin{block}{Idea}
Employ the kernel trick to eliminate the supremum in the expression for the integral probability metric.
\end{block}

\end{frame}

\begin{frame}
\frametitle{Reproducing kernel Hilbert space}

A \textit{Hilbert space} is a vector space $V$ equipped with the inner product operation $\langle \cdot, \cdot \rangle$ and its induced norm $\|\cdot\|$ satisfying $\|v\|^2 = \langle v, v \rangle$ for all $v \in V$, if it is complete:
\begin{equation*}
\sum_{i=1}^\infty \|v_i\| < \infty \quad \text{implies} \quad \sum_{i=1}^\infty v_i \in V
\end{equation*}
for any sequence $v_i \in V$.

A Hilbert space $\mathcal{H}$ of real-valued functions defined on a set $\mathcal{X}$ is called a \textit{reproducing kernel Hilbert space (RKHS)} if there exists a function $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ such that:
\begin{itemize}
\item for every $x \in \mathcal{X}$, the function $k(x, \cdot)$ belongs to $\mathcal{H}$,
\item $k$ satisfies the reproducing property $\langle f(\cdot), k(\cdot, x)\rangle = f(x)$ for any $f \in \mathcal{H}$ and $x \in \mathcal{X}$.
\end{itemize}

We denote $\mathcal{H}(k)$ the RKHS with kernel $k$.

\end{frame}

\begin{frame}
\frametitle{Kernel Stein discrepancy}

Taking the unit-ball in a Cartesian product of $d$ copies $\mathcal{H}(k)$
\begin{equation*}
\mathcal{G} \coloneq \left\{ \mathrm{g} : \mathbb{R}^d \to \mathbb{R}^d \left| \sum_{i=1}^d \|g_i\|^2_{\mathcal{H}(k)} \leq 1 \right.\right\},
\label{eq:unit-ball}
\end{equation*}
Proposition~2 in \cite{gorhamMeasuringSampleQuality2017} shows that the Stein discrepancy becomes
\begin{equation*}
\mathcal{D}_{P}^2(P') \coloneq \mathcal{D}_{P, \mathcal{G}}(P') = \iint_\mathcal{X} k_P(x, y) \diff p'(x) \diff p'(y),
\label{eq:stein-discrepancy-sqrt-expectation}
\end{equation*}
where $p'$ is the density of $P'$, and $k_P(x, y)$ is given by
\begin{equation*}
\begin{aligned}
k_P(x, y) \coloneq 
&(\nabla_x\cdot\nabla_y) k(x,y) \\
&+ \langle \nabla_x k(x, y), \nabla_y \log p(y) \rangle + \langle \nabla_y k(x, y), \nabla_x \log p(x) \rangle \\
&+ k(x, y) \langle \nabla_x \log p(x), \nabla_y \log p(y) \rangle.
\label{eq:deriv:stein-kernel}
\end{aligned}
\end{equation*}

\end{frame}

\begin{frame}
\frametitle{Kernel Stein discrepancy (continued)}

\begin{block}{Kernel Stein discrepancy (KSD)}
\begin{equation*}
\mathcal{D}_{P}^2(P') \coloneq \iint_\mathcal{X} k_P(x, y) \diff p'(x) \diff p'(y),
\end{equation*}

\end{block}

If $P'$ is the discrete distribution, the evaluation of KSD is a straightforward average of elements in the Gram matrix of the kernel $k_P$:
\begin{equation*}
\mathcal{D}_{P}^2\left(\frac{1}{n} \sum_{i=1}^n \delta(x_i)\right) = \frac{1}{n^2} \sum_{i,j=1}^n k_P(x_i, x_j),
\label{eq:ksd:discrete}
\end{equation*}

\end{frame}

\begin{frame}
\frametitle{Inverse multiquadric kernel}

The common choice of the kernel $k$ is the inverse multiquadric kernel (IMQ)
\begin{equation*}
k(x, y) = \left(c^2 + \|\Gamma^{-1/2}(x-y)\|\right)^\beta.
\end{equation*}

When $\beta \in (-1, 0)$ and $\Gamma = I$, \cite{gorhamMeasuringSampleQuality2017} demonstrate that $\mathcal{D}_{P}(P')$ provides convergence control (Theorem 8). Theorem 4 in \cite{chenSteinPointMarkov2019} justifies the introduction of $\Gamma$ in IMQ.

\end{frame}

\begin{frame}
\frametitle{Stein thinning}

\cite{riabizOptimalThinningMCMC2022} propose a greedy algorithm to select points from the sample that minimise the KSD at each iteration:

\begin{algorithm}[H]
\caption{Stein thinning.}\label{alg:cap}
\KwData{
\begin{itemize}
\item[] sample $(x_i)_{i=1}^n$ from MCMC,
\item[] gradients $(\nabla \log p(x_i))_{i=1}^n$
\item[] desired cardinality $m \in \mathbb{N}$
\end{itemize}
}
\KwResult{Indices $\pi$ of a sequence $(x_{\pi(j)})_{j=1}^m$ where $\pi(j) \in \{1, \dots, n\}$.}

\For{$j = 1, \dots, m$}{
$$\pi(j) \in \argmin_{i=1,\dots,n} \frac{k_P(x_i, x_i)}{2} + \sum_{j'=1}^{j-1} k_P(x_{\pi(j')}, x_i)$$
}
\end{algorithm}

\end{frame}

\subsection{Gradient-free kernel Stein discrepancy}

\section{Methodology}

\subsection{Proposed algorithm}

\begin{frame}

\begin{algorithm}[H]
\caption{Gradient-free Stein thinning.}\label{alg:gf}
\KwData{
\begin{itemize}
\item[] sample $(x_i)_{i=1}^n$ from MCMC,
\item[] target log-densities $(\log p(x_i))_{i=1}^n$
\item[] auxiliary log-densities $(\log q(x_i))_{i=1}^n$
\item[] auxiliary gradients $(\nabla \log q(x_i))_{i=1}^n$
\item[] desired cardinality $m \in \mathbb{N}$
\end{itemize}
}
\KwResult{Indices $\pi$ of a sequence $(x_{\pi(j)})_{j=1}^m$ where $\pi(j) \in \{1, \dots, n\}$.}

\For{$j = 1, \dots, m$}{
$$\pi(j) \in \argmin_{i=1,\dots,n} \frac{k_{P,Q}(x_i, x_i)}{2} + \sum_{j'=1}^{j-1} k_{P,Q}(x_{\pi(j')}, x_i)$$
}
\end{algorithm}

\end{frame}

\begin{frame}

\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Optimised gradient-free Stein thinning.}\label{alg:gf:optimised}
\KwData{
\begin{itemize}
\item[] sample $(x_i)_{i=1}^n$ from MCMC,
\item[] target log-densities $(\log p(x_i))_{i=1}^n$
\item[] auxiliary log-densities $(\log q(x_i))_{i=1}^n$
\item[] auxiliary gradients $(\nabla \log q(x_i))_{i=1}^n$
\item[] desired cardinality $m \in \mathbb{N}$.
\end{itemize}
}
\KwResult{Indices $\pi$ of a sequence $(x_{\pi(j)})_{j=1}^m$ where $\pi(j) \in \{1, \dots, n\}$.}
Initialise an array $A[i]$ of size $n$ \;
Set $A[i] = k_{P,Q}(x_i, x_i)$ for $i = 1, \dots, n$\;
Set $\pi(1) = \argmin_i A[i]$ \;

\For{$j = 2, \dots, m$}{
Update $A[i] = A[i] + 2 k_{P,Q}(x_{\pi(j - 1)}, x_i)$ for $i = 1, \dots, n$\;
Set $\pi(j) = \argmin_i A[i]$ \;
}
\end{algorithm}

\end{frame}

\subsection{Energy distance}

\section{Results}

\subsection{Bivariate Gaussian mixture}

\subsection{Lotka-Volterra inverse problem}

\section{Conclusions}

\begin{frame}
\frametitle{Contribution}

The project makes three contributions:
\begin{itemize}
\item implementation of the gradient-free Stein thinning algorithm in the Python library \texttt{stein-thinning},
\item evaluation of the performance of the proposed algorithm,
\item improvement of the computational efficiency of the existing Stein thinning algorithm from $O(nm^2)$ to $O(nm)$, where $n$ is the input sample size and $m$ is the desired thinned sample size.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Conclusions}

\begin{itemize}
\item The gradient-free approach is feasible and performs similarly to the Stein thinning algorithm of \cite{riabizOptimalThinningMCMC2022} for small thinned sample sizes,
\item The performance of the algorithm depends crucially on the choice of the auxiliary distribution. For example, even in the highly favourable setting of i.i.d.\ samples from a Gaussian mixture, choosing the auxiliary distribution based on the Laplace approximation fails to produce a thinned sample.
\item The simple multivariate Gaussian distribution using the sample mean and covariance offered a good starting point in our experiments, however bespoke treatment might be required for more complex problems.
\item In deciding whether to use the new algorithm as opposed to the gradient-based approach, the effort involved in selecting a good auxiliary distribution must be weighed against the computational cost of obtaining gradients.
\end{itemize}

\end{frame}

\section{Further Research}

\begin{frame}
\frametitle{Further Research}
\begin{itemize}

\item Evaluate the choices of KDE kernels other than Gaussian for constructing the auxiliary distribution.

\item Parallelise the computation of KDE.

\item Perform thinning in a lower-dimensional space.

\item Investigate the behaviour of Stein thinning for large thinned sample sizes.

\item Compare the performance of the approaches in terms of estimating the true parameters of the Lotka-Volterra model.

\item Run an experiment with randomised starting points.

\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Further Research (continued)}

\begin{itemize}

\item Repeat the experiments with more advanced MCMC algorithms. 

\item Check how running a gradient-free MCMC sampling algorithm (such the random-walk Metropolis-Hastings) followed by Stein thinning of the sample compares to running a gradient-based sampling algorithm (e.g. HMC).

\item Provide theoretical justification for gradient-free Stein thinning.

\item Explore other gradient-free alternatives.

\end{itemize}

\end{frame}

\section{References}

\begin{frame}[allowframebreaks]
\frametitle{References}

\fontsize{10pt}{12}\selectfont

\bibliographystyle{../report/plainnat_modified}
\bibliography{../report/biblio}

\end{frame}

\end{document}