Gradient-Free Optimal Postprocessing
of MCMC Output
by
Artem Glebov

Department of Mathematics
King’s College London
The Strand, London WC2R 2LS
United Kingdom

Abstract

1

Contents
1 Background

4

1.1

Markov chain Monte Carlo . . . . . . . . . . . . . . . . . . . .

4

1.2

Challenges of running MCMC . . . . . . . . . . . . . . . . . .

6

1.3

Optimal thinning . . . . . . . . . . . . . . . . . . . . . . . . .

8

1.3.1

Kernel Stein discrepancy . . . . . . . . . . . . . . . . .

8

1.3.2

Stein thinning . . . . . . . . . . . . . . . . . . . . . . . 11

2 Methodology

13

2.1

Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

2.2

Evaluating the approximation . . . . . . . . . . . . . . . . . . 13

3 Results

14

4 Conclusions

15

A Derivations

16

A.1 Stein kernel based on inverse multiquadratic kernel . . . . . . 16

2

Introduction
The Python code accompanying this report can be found at:
https://github.com/aglebov/gradient-free-mcmc-postprocessing.
The implementation of the gradient-free kernel Stein thinning was contributed by the author directly to the stein-thinning Python library:
https://github.com/wilson-ye-chen/stein_thinning.

3

Chapter 1
Background
1.1

Markov chain Monte Carlo

Markov chain Monte Carlo (MCMC) are a popular class of algorithms for
sampling from complex probability distributions.
The need to sample from a probability distribution arises in exploratory
analysis as well as when analytical expressions are unavailable for quantities of interest, such as the modes or quantiles of the distribution, or for
expectations with respect to the distribution, so a numerical simulation is
used to obtain approximations instead. Such cases are frequent in Bayesian
analysis, where the posterior density often has a complex structure with an
analytically intractable normalising constant.
Describe alternatives: the inverse method, accept-reject and importance
sampling
Include a simple motivating example
An MCMC algorithm proceeds by sequentially constructing a chain of
samples1 x1 , x2 , . . . , where each sample is drawn from a transition distribution Q conditional on the preceding value:
xn+1 ∼ Q(xn+1 |xn ).
The distribution Q is known as the transition kernel and is selected so that
1

These are also sometimes called “draws”. In this report, we follow the literature in
using the term “sample” both for a single element in an MCMC chain and for all such
elements taken together as a sample from the target distribution.

4

it is easy to sample from and to ensure asymptotic convergence to the target
distribution Π:
d
xn →
− Π as n → ∞.
Two classical variations of this technique are the Metropolis-Hastings
and Gibbs algorithms.
Metropolis-Hastings algorithm. The algorithm due to Metropolis et al.
(1953) and Hastings (1970) uses an auxiliary distribution q to sample a proposed value
x′ ∼ q(x′ |xn ),
which is then accepted with probability
π(x′ ) q(xn |x′ )
.
α(xn , x′ ) = 1 ∧
π(xn ) q(x′ |xn )
If x′ is accepted, the algorithm sets xn+1 = x′ . If x′ is rejected, the value
remains unchanged: xn+1 = xn .
Consider using a different notation to avoid the confusion between the
density of the proposal q and the transition kernel Q.
The common choice for the proposal distribution q is a symmetric proposal satisfying q(x′ |xn ) = q(xn |x′ ), so that the ratio of these two quantities
disappears from the expression for the acceptance probability:
π(x′ )
.
α(xn , x′ ) = 1 ∧
π(xn )
In the special case where q(x′ |xn ) = q(x′ − xn ) we obtain a random walk
proposal:
x′ = xn + Z,
where Z is the distribution of the step taken by the algorithm, e.g. a multivariate normal distribution. The operation of the algorithm then resembles
a random walk across the domain of the target distribution where steps towards areas of lower probability are more likely to be rejected. The scale
of the step distribution Z determines the average size of the jump that the
algorithm can make in one iteration and thus the speed of traversal of the
target domain.
An alternative to symmetric proposals is an independence proposal satisfying q(x′ |xn ) = q(x′ ).
Cite the ST03 lecture notes or Robert & Casella
5

Gibbs algorithm. Suppose x is a d-dimensional vector and the components x(1) , x(2) , . . . , x(d) can be partitioned in such a way that we can sample
the components belonging to each partition while keeping the components in
other partitions fixed. That is, let Ii ⊂ {1, . . . , d} with ∪ki=1 Ii = {1, . . . , d}
for some k and Ii ∩ Ij = ∅ for i ̸= j, and assume we can sample

x(Ii ) ∼ fi x(Ii ) |x(I1 ,...,Ii−1 ,Ii+1 ,...,Ik ) .
The sample xn+1 can then be constructed by sequentially sampling for each
partition:


(I1 ,...,Ii−1 )
(Ii )
i+1 ,...,Ik )
xn+1
∼ fi x(Ii ) |xn+1
, x(I
.
n
(I ,...,Ii−1 )

1
Note that the newly sampled values xn+1
subsequent partitions.

enter the computation for

Read and cite the original paper for Gibbs sampler
Consider simplifying this description
Mention HMC and other recent variations

1.2

Challenges of running MCMC

While the asymptotic convergence of MCMC samples to the target distribution is guaranteed, no general guarantee is available for finite samples,
resulting in several interrelated challenges that a practitioner faces when applying this class of algorithms:
1. The choice of a starting point for a chain affects the speed of convergence to the target distribution.
2. For a multimodal distribution, the algorithm might struggle to move
between the modes within a feasible time. This problem becomes especially acute in high dimensions.
3. The scale of the proposal distribution must be calibrated to ensure that
the algorithm is able to explore the domain of the target distribution
efficiently.
4. Assessing how close an MCMC chain is to convergence is difficult, since
the knowledge about the target distribution often comes from the chain
itself.
6

5. In order to eliminate the impact of the starting point, it can be useful to
discard the initial iterations of an MCMC chain, which are considered
as “burn-in”. Selecting the optimal length of the burn-in period is
contingent on being able to detect convergence.
6. The sequential procedure of constructing a chain induces autocorrelation between the samples, which leads to increased variance of derived
estimators.
7. The large number of samples resulting from an MCMC algorithm needs
to be summarised for subsequent analysis, particularly when the cost
of using all available samples is too high. Such situations arise when
samples obtained from MCMC are used as starting points for further
expensive simulations.
The first three challenges require decisions to be made upfront before
running the algorithm or adaptively during its run. In order to address the
impact of the starting point, running multiple chains with starting points
sampled from an overdispersed distribution is recommended (Gelman and
Rubin (1992)). This approach has the added benefit of increasing the chance
of discovering the modes of the target distribution, although it does not
provide a guarantee in this respect.
Mention perfect sampling.
Comparing the summary statistics of several chains (Gelman and Rubin (1992); Brooks and Gelman (1998); Vehtari et al. (2021)) offers a way to
detect a lack of convergence at the cost of additional computation. Alternatively, the comparison can be applied to batches of samples from a single
chain, as proposed by Vats and Knudson (2021). Convergence detection can
be used to terminate the algorithm once a chosen criterion is satisfied, or
to assess the quality of a sample retrospectively. It should be noted that
convergence criteria establish a necessary but not sufficient condition for
convergence, so the outcomes need to be interpreted accordingly.
The scaling of the step distribution in a random-walk Metropolis-Hastings
algorithm is commonly tuned to target the acceptance rate of roughly 0.234
for proposed samples (Gelman et al. (1996, 1997); Roberts and Rosenthal
(2001)), which balances the speed of traversal and the computational effort
generating samples that end up rejected.
The last three challenges are typically addressed by post-processing a
sample from a completed MCMC run. A recent proposal by Riabiz et al.
7

(2022) addresses these challenges by selecting a fixed-size subset of samples
from an MCMC run such that the empirical distribution given by the subset
best approximates the distribution resulting from the full sample. In the
following section, we consider their approach in greater detail.
Read and cite Cowles and Carlin (1996) regarding the choice of burn-in
length.

1.3

Optimal thinning

Given a Markov chain (Xi )i∈N and its realisation of length n, the empirical
approximation of the target posterior distribution is given by
n

1X
δ(Xi ),
n i=1
where δ(x) is the Dirac delta function. Riabiz et al. (2022) set out to identify m ≪ n indices π(j) ∈ {1, . . . , n} with j ∈ {1, . . . , m}, such that the
approximation provided by the subset of samples
m

1 X
δ(Xπ(j) )
m j=1

(1.1)

is closest to the approximation given by the full set, and thus to the target
distribution. The criterion of proximity is based on the kernel Stein discrepancy, itself a special case of the integral probability metric.

1.3.1

Kernel Stein discrepancy

The integral probability metric between two distributions P and Q is defined
as
Z
Z
DF (P, Q) := sup
f dP −
f dQ ,
(1.2)
f ∈F

X

X

where X is a measurable space on which both P and Q are defined and F
is a set of test functions. Depending on the choice of F, the definition (1.2)
gives rise to different classes of probability metrics, including the well-known
Kolmogorov distance, Wasserstein distance and total variation distance.
If P is taken to be the target distribution of an MCMC algorithm,
evaluating (1.2) poses two practical challenges:
8

• the integral

R
X

f dP is often analytically intractable,

• the supremum requires a non-trivial optimisation procedure to find.
The need to integrate with
respect to P can be eliminated if we find a
R
set of function F for which X f dP = 0 for all f ∈ F. The expression (1.2)
then simplifies to
Z
DF (P, Q) = sup

f dQ .

f ∈F

(1.3)

X

Gorham and Mackey (2015) propose choosing such a set F based on the
observation that the infinitesimal generator
E[u(Zt )|Z0 = x] − u(x)
t→0
t

(Lu)(x) = lim

for u : Rd → R

of a Markov process (Zt )t≥0 with stationary distribution P satisfies
E[(Lu)(Z)] = 0
under mild conditions on L and u
Check the conditions.
. In the specific case of an overdamped Langevin diffusion
1
dZt = ∇ log p(Zt ) dt + dWt ,
2
where Wt is the standard Brownian motion, the infinitesimal generator becomes
1
1
(LP u)(x) = ⟨∇u(x), ∇ log p(x)⟩ + ⟨∇, ∇u(x)⟩.
2
2
1
Denoting g = 2 ∇u, they obtain the Stein operator
AP g = ⟨g, ∇ log p⟩ + ⟨∇, g⟩ = ⟨p−1 ∇, pg⟩,

(1.4)

and rewrite (1.3) as
Z
DG (P, Q) = sup
g∈G

AP g dQ

(1.5)

X

for a suitably chosen set G. Note that p enters (1.4) via ∇ log p, so the
knowledge of its normalising constant is not required to evaluate the operator.
9

To remove the optimisation step in the calculation of (1.5), Gorham
and Mackey (2017) employ a reproducing kernel Hilbert space (RKHS) H(k)
with kernel k : Rd × Rd → R satisfying the reproducing property:
f (x) = ⟨f, k(x, ·)⟩ for f ∈ H(k).
Taking the set
(
G :=

g : Rd → Rd

d
X

)
∥gi ∥2H(k) ≤ 1

i=1

which defines a unit-ball in a Cartesian product of d copies H(k), Proposition
2 in Gorham and Mackey (2017) establishes that
q
(1.6)
DG (P, Q) = EQ×Q [kP (X, X̃)],
where X, X̃ ∼ Q and kP (x, y) is given by
kP (x, y) :=(∇x · ∇y )k(x, y)
+ ⟨∇x k(x, y), ∇y log p(y)⟩ + ⟨∇y k(x, y), ∇x log p(x)⟩
+ k(x, y)⟨∇x log p(x), ∇y log p(y)⟩.

(1.7)

Here ∇x and ∇y are gradients w.r.t. x and y and the operator ∇x · ∇y is
defined as:
d
X
∂2
(∇x · ∇y )k(x, y) :=
k(x, y).
∂x
∂y
i
i
i=1
For a discrete distribution Q, as in the approximation (1.1), the discrepancy (1.6) becomes
v
u
m
u 1 X
kP (Xπ(i) , Xπ(j) ).
(1.8)
DG (P, Q) = t 2
m i,j=1
When k(x, y) is chosen to be the inverse multiquadratic kernel (IMQ)
β
k(x, y) = c2 + ∥Γ−1/2 (x − y)∥
(1.9)
with β ∈ (−1, 0), Gorham and Mackey (2017) demonstrate for Γ = I that
DG (P, Q) provides convergence control:
• if DG (P, Qm ) → 0, then Qm converges in distribution to P (Theorem
8),
10

• if Qm converges to P in Wasserstein distance, then DG (P, Qm ) → 0
(Proposition 9)
for a sequence of distributions Qm under suitable conditions. Theorem 4 by
Chen et al. (2019) justifies the introduction of Γ in IMQ.
The constant c in (1.9) can be set to 1 without loss of generality, and the
positive-definite preconditioner matrix Γ can be chosen to exploit the geometry of the parameter space. Riabiz et al. (2022) suggest several choices for
Γ, in particular the identity matrix scaled by the median Euclidean distance
between points in the sample, possibly further rescaled by (log m)−1/2 where
m is the desired cardinality of the thinned sample, or the sample covariance
matrix. Based on their empirical evaluation, Gorham and Mackey (2017)
and Riabiz et al. (2022) settle on the value β = − 12 .
The expression for kP (x, y) when k(x, y) is taken to be the inverse multiquadratic kernel is derived in section A.1 and is coded directly in the Python
library stein-thinning.

1.3.2

Stein thinning

Mention Chen (2018b) as the source of the greedy algorithm idea
Equipped with the kernel Stein discrepancy (KSD) as defined above,
Riabiz et al. (2022) develop a greedy optimisation algorithm to select a subset
of points from a sample that minimises the total kernel
Stein discrepancy.

n
Rather than attempting to evaluate KSD for all m combinations of points,
they construct a subsample iteratively, selecting a point that minimises the
KSD with previously selected points. We reproduce their procedure verbatim
in Algorithm 1 for the reader’s convenience.
The algorithm was implemented by the authors in the open-source library stein-thinning2 .

2

Available from https://github.com/wilson-ye-chen/stein_thinning.

11

Data: Sample (xi )ni=1 from MCMC, Stein kernel kP , desired
cardinality m ∈ N.
n
Result: Indices π of a sequence (xπ(j) )m
j=1 ⊂ {xi }i=1 where
π(j) ∈ {1, . . . , n}.
for j = 1, . . . , m do
j−1
kP (xi , xi ) X
+
kP (xπ(j ′ , xi )
π(j) ∈ arg min
2
i=1,...,n
j ′ =1
end
Algorithm 1: Stein thinning

12

Chapter 2
Methodology
2.1

Data

2.2

Evaluating the approximation

In order to assess how well the selected sample approximates the posterior
distribution, we use the energy distance. Following Rizzo and Székely (2016),
the squared energy distance is defined for two distributions F and G as
D2 (F, G) := 2E∥X − Y ∥ − E∥X − X ′ ∥ − E∥Y − Y ′ ∥,
where X, X ′ ∼ F , Y, Y ′ ∼ G, and ∥ · ∥ denotes the Euclidean norm. For samples x1 , . . . , xn and y1 , . . . , ym from X and Y , respectively, the corresponding
statistic is given by
n
m
n
n
m m
1 XX
1 XX
2 XX
∥xi −yj ∥− 2
∥xi −xj ∥− 2
∥yi −yj ∥.
En,m (X, Y ) :=
nm i=1 j=1
n i=1 j=1
m i=1 j=1

13

Chapter 3
Results

14

Chapter 4
Conclusions

15

Appendix A
Derivations
A.1

Stein kernel based on inverse multiquadratic
kernel

Given a kernel k(x, y), the corresponding Stein kernel is given by (1.7). For
the inverse multiquadratic kernel (1.9) we obtain
∂
k(x, y) = β
∂xr
×

d
X

c2 +

d X
d
X

!β−1
(xi − yi )Γ−1
ij (xj − yj )

i=1 j=1

(Γ−1 + Γ−T )rj (xj − yj )

j=1

= 2β

2

c +

d X
d
X

!β−1
(xi − yi )Γ−1
ij (xj − yj )

i=1 j=1

d
X

Γ−1
rj (xj − yj ),

j=1

(A.1)
where we used that Γ is a symmetric matrix. The gradient is then
∇x k(x, y) = 2β c2 + ∥Γ−1/2 (x − y)∥2

β−1

Γ−1 (x − y).

(A.2)

and similarly
∇y k(x, y) = −2β c2 + ∥Γ−1/2 (x − y)∥2

16

β−1

Γ−1 (x − y).

(A.3)

Now
d X
d
X
∂2
2
k(x, y) = −4β(β − 1) c +
(xi − yi )Γ−1
ij (xj − yj )
∂xr ∂yr
i=1 j=1
!2
d
X
×
Γ−1
rj (xj − yj )

!β−2

j=1

− 2β

c2 +

d X
d
X

!β−1
(xi − yi )Γ−1
ij (xj − yj )

Γ−1
rr

i=1 j=1

(A.4)
which gives us
β−2 −1
∥Γ (x − y)∥2
(∇x · ∇y )k(x, y) = −4β(β − 1) c2 + ∥Γ−1/2 (x − y)∥2
β−1
− 2β c2 + ∥Γ−1/2 (x − y)∥2
trace(Γ−1 )
(A.5)
Substituting (A.2), (A.3) and (A.5) into (1.7), we obtain
β−2 −1
kP (x, y) = − 4β(β − 1) c2 + ∥Γ−1/2 (x − y)∥2
∥Γ (x − y)∥2
β−1
− 2β c2 + ∥Γ−1/2 (x − y)∥2
trace(Γ−1 )
β−1 −1
+ 2β c2 + ∥Γ−1/2 (x − y)∥2
⟨Γ (x − y), ∇y log p(y)⟩

β−1
− 2β c2 + ∥Γ−1/2 (x − y)∥2
⟨Γ−1 (x − y), ∇x log p(x)⟩
β
+ c2 + ∥Γ−1/2 (x − y)∥2 ⟨∇x log p(x), ∇y log p(y)⟩
= − 4β(β − 1)Dβ−2 ∥Γ−1 (x − y)∥2
− 2βDβ−1 (trace(Γ−1 ) + ⟨Γ−1 (x − y), ∇x log p(x) − ∇y log p(y)⟩)
+ Dβ ⟨∇x log p(x), ∇y log p(y)⟩,
(A.6)
where we have denoted D = c2 + ∥Γ−1/2 (x − y)∥2 .

17

Bibliography
S. P. Brooks and A. Gelman. General Methods for Monitoring Convergence of
Iterative Simulations. Journal of Computational and Graphical Statistics,
7(4):434–455, December 1998.
W. Y. Chen, A. Barp, F.-X. Briol, J. Gorham, M. Girolami, L. Mackey, and
Chris. J. Oates. Stein Point Markov Chain Monte Carlo. In Proceedings
of the 36th International Conference on Machine Learning, Long Beach,
California, 2019. PLMR.
A. Gelman and D. B. Rubin. Inference from Iterative Simulation Using
Multiple Sequences. Statistical Science, 7(4), November 1992.
A. Gelman, G. O. Roberts, and W. R. Gilks. Efficient Metropolis Jumping
Rules. In Bayesian Statistics, volume 5, pages 599–608. Oxford University
Press, Oxford, May 1996.
A. Gelman, W. R. Gilks, and G. O. Roberts. Weak convergence and optimal
scaling of random walk Metropolis algorithms. The Annals of Applied
Probability, 7(1), February 1997.
J. Gorham and L. Mackey. Measuring Sample Quality with Stein’s Method.
In Advances in Neural Information Processing Systems, volume 28. MIT
Press, 2015.
J. Gorham and L. Mackey. Measuring Sample Quality with Kernels. In
Proceedings of the 34th International Conference on Machine Learning,
Sydney, Australia, 2017. PLMR.
W. K. Hastings. Monte Carlo sampling methods using Markov chains and
their applications. Biometrika, 57(1):97–109, April 1970.
N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and
E. Teller. Equation of State Calculations by Fast Computing Machines.
The Journal of Chemical Physics, 21(6):1087–1092, June 1953.
18

M. Riabiz, W. Y. Chen, J. Cockayne, P. Swietach, S. A. Niederer, L. Mackey,
and Chris. J. Oates. Optimal Thinning of MCMC Output. Journal of
the Royal Statistical Society Series B: Statistical Methodology, 84(4):1059–
1081, September 2022.
M. L. Rizzo and G. J. Székely. Energy distance. WIREs Computational
Statistics, 8(1):27–38, January 2016.
G. O. Roberts and J. S. Rosenthal. Optimal scaling for various MetropolisHastings algorithms. Statistical Science, 16(4), November 2001.
D. Vats and C. Knudson. Revisiting the Gelman–Rubin Diagnostic. Statistical Science, 36(4), November 2021.
A. Vehtari, A. Gelman, D. Simpson, B. Carpenter, and P.-C. Bürkner. RankNormalization, Folding, and Localization: An Improved R for Assessing
Convergence of MCMC (with Discussion). Bayesian Analysis, 16(2), June
2021.

19

