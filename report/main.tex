\documentclass[12pt,a4paper]{report}
\usepackage{graphicx,epsfig}
\usepackage{amsfonts}
\usepackage{amsthm, amsmath}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{listings}
\usepackage[authoryear,round]{natbib}
\usepackage[hidelinks]{hyperref}
\usepackage{todonotes}
\usepackage{mathtools}

\newcommand*\diff{\mathop{}\!\mathrm{d}}
\DeclareMathOperator{\trace}{trace}

\parindent=25pt
\parskip 0.10in

\date{}
\begin{document}
\thispagestyle{empty}
\begin{center}
{\huge
Gradient-Free Optimal Postprocessing of MCMC Output

\bigskip
\bigskip

by
\bigskip
\bigskip

Artem Glebov
}
\end{center}
\vfill

\begin{center}
{\large
Department of Mathematics\\
King's College London\\
The Strand, London WC2R 2LS\\
United Kingdom\\
\medskip

}
\end{center}
\bigskip


\newpage
\setcounter{page}{1}

\chapter*{Abstract}


\tableofcontents


\chapter*{Introduction}

\chapter{Background}

\section{Markov chain Monte Carlo}

Markov chain Monte Carlo (MCMC) are a popular class of algorithms for sampling from complex probability distributions.

The need to sample from a probability distribution arises in exploratory analysis as well as when analytical expressions are unavailable for quantities of interest, such as the modes or quantiles of the distribution, or for expectations with respect to the distribution, so a numerical simulation is used to obtain approximations instead. Such cases are frequent in Bayesian analysis, where the posterior density often has a complex structure with an analytically intractable normalising constant.

\todo[inline]{Describe alternatives: the inverse method, accept-reject and importance sampling}

\todo[inline]{Include a simple motivating example}

An MCMC algorithm proceeds by sequentially constructing a chain of samples\footnote{These are also sometimes called ``draws''. In this report, we follow the literature in using the term ``sample'' both for a single element in an MCMC chain and for all such elements taken together as a sample from the target distribution.} $x_1$, $x_2$, $\dots$,  where each sample is drawn from a transition distribution $Q$ conditional on the preceding value:
$$x_{n+1} \sim Q(x_{n+1}|x_n).$$
The distribution $Q$ is known as the transition kernel and is selected so that it is easy to sample from and to ensure asymptotic convergence to the target distribution $\Pi$:
$$x_n \xrightarrow[]{d} \Pi \quad\text{as}\quad n \to \infty.$$

Two classical variations of this technique are the Metropolis-Hastings and Gibbs algorithms.

\paragraph{Metropolis-Hastings algorithm.} The algorithm due to \cite{metropolisEquationStateCalculations1953} and \cite{hastingsMonteCarloSampling1970} uses an auxiliary distribution $q$ to sample a proposed value
$$x' \sim q(x' | x_n),$$
which is then accepted with probability
$$\alpha(x_n, x') = 1 \wedge \frac{\pi(x')}{\pi(x_n)} \frac{q(x_n|x')}{q(x'|x_n)}.$$
If $x'$ is accepted, the algorithm sets $x_{n+1} = x'$. If $x'$ is rejected, the value remains unchanged: $x_{n+1} = x_n$.

\todo[inline]{Consider using a different notation to avoid the confusion between the density of the proposal $q$ and the transition kernel $Q$.}

The common choice for the proposal distribution $q$ is a symmetric proposal satisfying $q(x'|x_n) = q(x_n|x')$, so that the ratio of these two quantities disappears from the expression for the acceptance probability:
$$\alpha(x_n, x') = 1 \wedge \frac{\pi(x')}{\pi(x_n)}.$$
In the special case where $q(x'|x_n) = q(x' - x_n)$ we obtain a random walk proposal:
$$x' = x_n + Z,$$
where $Z$ is the distribution of the step taken by the algorithm, e.g. a multivariate normal distribution. The operation of the algorithm then resembles a random walk across the domain of the target distribution where steps towards areas of lower probability are more likely to be rejected. The scale of the step distribution $Z$ determines the average size of the jump that the algorithm can make in one iteration and thus the speed of traversal of the target domain.

An alternative to symmetric proposals is an independence proposal satisfying $q(x'|x_n) = q(x')$.

\todo[inline]{Cite the ST03 lecture notes or Robert \& Casella}

\paragraph{Gibbs algorithm.} Suppose $x$ is a $d$-dimensional vector and the components $x^{(1)}$, $x^{(2)}$, $\dots$, $x^{(d)}$ can be partitioned in such a way that we can sample the components belonging to each partition while keeping the components in other partitions fixed. That is, let $I_i \subset \{1, \dots, d\}$ with $\cup_{i=1}^k I_i = \{1, \dots, d\}$ for some $k$ and $I_i \cap I_j = \emptyset$ for $i \neq j$, and assume we can sample
$$x^{(I_i)} \sim f_i\left(x^{(I_i)} | x^{(I_1, \dots, I_{i-1}, I_{i+1}, \dots, I_k)}\right).$$
The sample $x_{n+1}$ can then be constructed by sequentially sampling for each partition:
$$x_{n+1}^{(I_i)} \sim f_i\left(x^{(I_i)} | x_{n+1}^{(I_1, \dots, I_{i-1})}, x_n^{(I_{i+1}, \dots, I_k)}\right).$$
Note that the newly sampled values $x_{n+1}^{(I_1, \dots, I_{i-1})}$ enter the computation for subsequent partitions.

\todo[inline]{Read and cite the original paper for Gibbs sampler}

\todo[inline]{Consider simplifying this description}

\todo[inline]{Mention HMC and other recent variations}

\section{Challenges of running MCMC}

While the asymptotic convergence of MCMC samples to the target distribution is guaranteed, no general guarantee is available for finite samples, resulting in several interrelated challenges that a practitioner faces when applying this class of algorithms:
\begin{enumerate}
\item The choice of a starting point for a chain affects the speed of convergence to the target distribution.
\item For a multimodal distribution, the algorithm might struggle to move between the modes within a feasible time. This problem becomes especially acute in high dimensions.
\item The scale of the proposal distribution must be calibrated to ensure that the algorithm is able to explore the domain of the target distribution efficiently.
\item Assessing how close an MCMC chain is to convergence is difficult, since the knowledge about the target distribution often comes from the chain itself.
\item In order to eliminate the impact of the starting point, it can be useful to discard the initial iterations of an MCMC chain, which are considered as ``burn-in''. Selecting the optimal length of the burn-in period is contingent on being able to detect convergence.
\item The sequential procedure of constructing a chain induces autocorrelation between the samples, which leads to increased variance of derived estimators.
\item The large number of samples resulting from an MCMC algorithm needs to be summarised for subsequent analysis, particularly when the cost of using all available samples is too high. Such situations arise when samples obtained from MCMC are used as starting points for further expensive simulations.
\end{enumerate}

The first three challenges require decisions to be made upfront before running the algorithm or adaptively during its run. In order to address the impact of the starting point, running multiple chains with starting points sampled from an overdispersed distribution is recommended~(\cite{gelmanInferenceIterativeSimulation1992}). This approach has the added benefit of increasing the chance of discovering the modes of the target distribution, although it does not provide a guarantee in this respect. 
\todo[inline]{Mention perfect sampling.}

Comparing the summary statistics of several chains (\cite{gelmanInferenceIterativeSimulation1992,brooksGeneralMethodsMonitoring1998,vehtariRankNormalizationFoldingLocalization2021}) offers a way to detect a lack of convergence at the cost of additional computation. Alternatively, the comparison can be applied to batches of samples from a single chain, as proposed by \cite{vatsRevisitingGelmanRubin2021}. Convergence detection can be used to terminate the algorithm once a chosen criterion is satisfied, or to assess the quality of a sample retrospectively. It should be noted that convergence criteria establish a necessary but not sufficient condition for convergence, so the outcomes need to be interpreted accordingly.

The scaling of the step distribution in a random-walk Metropolis-Hastings algorithm is commonly tuned to target the acceptance rate of roughly 0.234 for proposed samples (\cite{gelmanEfficientMetropolisJumping1996,gelmanWeakConvergenceOptimal1997,robertsOptimalScalingVarious2001}), which balances the speed of traversal and the computational effort generating samples that end up rejected.

The last three challenges are typically addressed by post-processing a sample from a completed MCMC run. A recent proposal by~\cite{riabizOptimalThinningMCMC2022} addresses these challenges by selecting a fixed-size subset of samples from an MCMC run such that the empirical distribution given by the subset best approximates the distribution resulting from the full sample. In the following section, we consider their approach in greater detail.

\todo[inline]{Read and cite Cowles and Carlin (1996) regarding the choice of burn-in length.}

\section{Optimal thinning}

Given a Markov chain $(X_i)_{i \in \mathbb{N}}$ and its realisation of length $n$, the empirical approximation of the target posterior distribution is given by
$$\frac{1}{n} \sum_{i=1}^n \delta(X_i),$$
where $\delta(x)$ is the Dirac delta function.
\cite{riabizOptimalThinningMCMC2022} set out to identify $m \ll n$ indices $\pi(j) \in \{1,\dots, n\}$ with $j\in\{1, \dots, m\}$, such that the approximation provided by the subset of samples
\begin{equation}
\frac{1}{m} \sum_{j=1}^m \delta(X_{\pi(j)})
\label{eq:thinned-sample}
\end{equation}
is closest to the approximation given by the full set, and thus to the target distribution. The criterion of proximity is based on the kernel Stein discrepancy, itself a special case of the integral probability metric.

\subsection{Kernel Stein discrepancy}

The integral probability metric between two distributions $P$ and $Q$ is defined as
\begin{equation}
\mathcal{D}_{\mathcal{F}}(P, Q) \coloneq \sup_{f \in \mathcal{F}}\left|\int_\mathcal{X} f \diff P - \int_\mathcal{X} f \diff Q \right|,
\label{eq:ipm}
\end{equation}
where $\mathcal{X}$ is a measurable space on which both $P$ and $Q$ are defined and $\mathcal{F}$ is a set of test functions. Depending on the choice of $\mathcal{F}$, the definition~(\ref{eq:ipm}) gives rise to different classes of probability metrics, including the well-known Kolmogorov distance, Wasserstein distance and total variation distance.

If $P$ is taken to be the target distribution of an MCMC algorithm, evaluating~(\ref{eq:ipm}) poses two practical challenges:
\begin{itemize}
\item the integral $\int_\mathcal{X} f \diff P$ is often analytically intractable,
\item the supremum requires a non-trivial optimisation procedure to find.
\end{itemize}

The need to integrate with respect to $P$ can be eliminated if we find a set of function $\mathcal{F}$ for which $\int_\mathcal{X} f \diff P = 0$ for all $f \in \mathcal{F}$. The expression~(\ref{eq:ipm}) then simplifies to
\begin{equation}
\mathcal{D}_{\mathcal{F}}(P, Q) = \sup_{f \in \mathcal{F}}\left|\int_\mathcal{X} f \diff Q \right|.
\label{eq:stein-discrepancy-sup}
\end{equation}

\todo[inline]{Check Barbour (1988)}

\cite{gorhamMeasuringSampleQuality2015} propose choosing such a set $\mathcal{F}$ based on the observation that the infinitesimal generator
$$(\mathcal{L}u)(x) = \lim_{t \to 0} \frac{\mathbb{E}[u(Z_t) | Z_0 = x] - u(x)}{t} \quad \text{for } u:\mathbb{R}^d \to \mathbb{R}$$
of a Markov process $(Z_t)_{t \geq 0}$ with stationary distribution $P$ satisfies 
$$\mathbb{E}[(\mathcal{L} u)(Z)] = 0$$
under mild conditions on $\mathcal{L}$ and $u$. In the specific case of an overdamped Langevin diffusion
$$\diff Z_t = \frac{1}{2} \nabla \log p(Z_t) \diff t + \diff W_t,$$
where $W_t$ is the standard Brownian motion, the infinitesimal generator becomes
$$(\mathcal{L}_P u)(x) = \frac{1}{2} \langle \nabla u(x), \nabla \log p(x)\rangle + \frac{1}{2}\langle \nabla, \nabla u(x) \rangle.$$
Denoting $g  = \frac{1}{2}\nabla u$, they obtain the Stein operator
\begin{equation}
\mathcal{A}_P g = \langle g, \nabla \log p \rangle + \langle \nabla, g \rangle = \langle p^{-1}\nabla, p g \rangle,
\end{equation}
and rewrite~(\ref{eq:stein-discrepancy-sup}) as
\begin{equation}
\mathcal{D}_{\mathcal{G}}(P, Q) = \sup_{g \in \mathcal{G}}\left|\int_\mathcal{X} \mathcal{A}_P g \diff Q \right|
\label{eq:stein-discrepancy-g}
\end{equation}
for a suitably chosen set $\mathcal{G}$.

To remove the optimisation step in the calculation of~(\ref{eq:stein-discrepancy-g}), \cite{gorhamMeasuringSampleQuality2017} employ a reproducing kernel Hilbert space (RKHS) $\mathcal{H}(k)$ with kernel $k: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$ satisfying the reproducing property:
$$f(x) = \langle f, k(x, \cdot)\rangle \quad\text{for } f \in \mathcal{H}(k).$$
Taking the set
$$\mathcal{G} \coloneq \left\{ \mathrm{g} : \mathbb{R}^d \to \mathbb{R}^d \left| \sum_{i=1}^d \|g_i\|^2_{\mathcal{H}(k)} \leq 1 \right.\right\}$$
which defines a unit-ball in a Cartesian product of $d$ copies $\mathcal{H}(k)$, Proposition 2 in \cite{gorhamMeasuringSampleQuality2017} establishes that
\begin{equation}
\mathcal{D}_{\mathcal{G}}(P, Q) = \sqrt{\mathbb{E}_{Q \times Q}[k_P(X, \tilde{X})]},
\label{eq:stein-discrepancy-sqrt-expectation}
\end{equation}
where $X, \tilde{X} \sim Q$ and $k_P(x, y)$ is given by
\begin{equation}
\begin{aligned}
k_P(x, y) \coloneq 
&(\nabla_x\cdot\nabla_y) k(x,y) \\
&+ \langle \nabla_x k(x, y), \nabla_y \log p(y) \rangle + \langle \nabla_y k(x, y), \nabla_x \log p(x) \rangle \\
&+ k(x, y) \langle \nabla_x \log p(x), \nabla_y \log p(y) \rangle.
\label{eq:deriv:stein-kernel}
\end{aligned}
\end{equation}
Here $\nabla_x$ and $\nabla_y$ are gradients w.r.t.\ $x$ and $y$ and the operator $\nabla_x\cdot\nabla_y$ is defined as:
$$(\nabla_x\cdot\nabla_y) k(x,y) \coloneq \sum_{i=1}^d \frac{\partial^2}{\partial x_i\, \partial y_i} k(x, y).$$

When the distribution $Q$ is discrete, as in the approximation~(\ref{eq:thinned-sample}), the discrepancy~(\ref{eq:stein-discrepancy-sqrt-expectation}) becomes
\begin{equation}
\mathcal{D}_{\mathcal{G}}(P, Q) = \sqrt{\frac{1}{m^2} \sum_{i,j=1}^m k_P(X_{\pi(i)}, X_{\pi(j)})}.
\end{equation}

When $k(x, y)$ is chosen to be the inverse multiquadratic kernel
\begin{equation}
k(x, y) = \left(c^2 + \|\Gamma^{-1/2}(x-y)\|\right)^\beta
\end{equation}
with $\beta \in (-1, 0)$, \cite{gorhamMeasuringSampleQuality2017} demonstrate that $\mathcal{D}_{\mathcal{G}}(P, Q)$ provides convergence control:
\begin{itemize}
\item if $\mathcal{D}_{\mathcal{G}}(P, Q_m) \to 0$, then $Q_m$ converges in distribution to $P$ (Theorem 8),
\item if $Q_m$ converges to $P$ in Wasserstein distance, then $\mathcal{D}_{\mathcal{G}}(P, Q_m) \to 0$ (Proposition 9)
\end{itemize}
for a sequence of distributions $Q_m$ under suitable conditions.

\chapter{Methodology}

\section{Data}

\section{Evaluating the approximation}

In order to assess how well the selected sample approximates the posterior distribution, we use the energy distance. Following \cite{rizzoEnergyDistance2016}, the squared energy distance is defined for two distributions $F$ and $G$ as
$$D^2(F, G) \coloneq 2 \mathbb{E} \|X - Y\| - \mathbb{E}\|X - X'\| - \mathbb{E} \|Y - Y'\|,$$
where $X, X' \sim F$, $Y, Y' \sim G$, and $\|\cdot\|$ denotes the Euclidean norm. For samples $x_1, \dots, x_n$ and $y_1, \dots, y_m$ from $X$ and $Y$, respectively, the corresponding statistic is given by
$$\mathcal{E}_{n,m}(X, Y) \coloneq \frac{2}{nm}\sum_{i=1}^n \sum_{j=1}^m \|x_i - y_j\| - \frac{1}{n^2} \sum_{i=1}^n\sum_{j=1}^n \|x_i - x_j\| - \frac{1}{m^2} \sum_{i=1}^m \sum_{j=1}^m \|y_i - y_j\|.$$

\chapter{Results}

\chapter{Conclusions}

\appendix
\chapter{Derivations}
\label{appendix:derivations}

Given a kernel $k(x,y)$, the corresponding Stein kernel is given by~(\ref{eq:deriv:stein-kernel}).

\todo[inline]{Expand this to describe how the Stein operator translates to a new RKHS.}

For the inverse multiquadratic kernel
\begin{equation}
\begin{split}
k(x, y)
&= \left(c^2 + \|\Gamma^{-1/2}(x-y)\|\right)^\beta \\
&=\left(c^2 + (x-y)^T \Gamma^{-1}(x-y)\right)^\beta \\
&=\left(c^2 + \sum_{i=1}^d\sum_{j=1}^d (x_i-y_i) \Gamma^{-1}_{ij}(x_j-y_j)\right)^\beta \\
\end{split}
\end{equation}
we have
\begin{equation}
\begin{aligned}
\frac{\partial}{\partial x_r} k(x,y) 
%&= \beta \left(c^2 + \sum_{i=1}^d\sum_{j=1}^d (x_i-y_i) \Gamma^{-1}_{ij}(x_j-y_j)\right)^{\beta-1} \\
%&\times \left( \sum_{j=1}^d \Gamma^{-1}_{lj}(x_j - y_j) + \sum_{i=1}^d (x_i - x_j) \Gamma^{-1}_{il} \right) \\
&= \beta \left(c^2 + \sum_{i=1}^d\sum_{j=1}^d (x_i-y_i) \Gamma^{-1}_{ij}(x_j-y_j)\right)^{\beta-1} \\
&\times \sum_{j=1}^d (\Gamma^{-1} + \Gamma^{-T})_{rj}(x_j - y_j) \\
&= 2 \beta \left(c^2 + \sum_{i=1}^d\sum_{j=1}^d (x_i-y_i) \Gamma^{-1}_{ij}(x_j-y_j)\right)^{\beta-1}
\sum_{j=1}^d \Gamma^{-1}_{rj}(x_j - y_j), \\
\end{aligned}
\end{equation}
where we used that $\Gamma$ is a symmetric matrix. The gradient is then
\begin{equation}
\nabla_x k(x,y) = 2 \beta \left(c^2 + \| \Gamma^{-1/2} (x-y)\|^2\right)^{\beta-1} \Gamma^{-1} (x - y).
\label{eq:appx:deviv:nablax}
\end{equation}
and similarly
\begin{equation}
\nabla_y k(x,y) = -2 \beta \left(c^2 + \| \Gamma^{-1/2} (x-y)\|^2\right)^{\beta-1} \Gamma^{-1} (x - y).
\label{eq:appx:deviv:nablay}
\end{equation}
Now
\begin{equation}
\begin{aligned}
\frac{\partial^2}{\partial x_r\,\partial y_r} k(x,y) 
&= -4 \beta(\beta-1) \left(c^2 + \sum_{i=1}^d\sum_{j=1}^d (x_i-y_i) \Gamma^{-1}_{ij}(x_j-y_j)\right)^{\beta-2} \\
&\times \left(\sum_{j=1}^d \Gamma^{-1}_{rj}(x_j - y_j)\right)^2 \\
&- 2\beta \left(c^2 + \sum_{i=1}^d\sum_{j=1}^d (x_i-y_i) \Gamma^{-1}_{ij}(x_j-y_j)\right)^{\beta-1} \Gamma^{-1}_{rr}
\end{aligned}
\end{equation}
which gives us
\begin{equation}
\begin{aligned}
\nabla_x \cdot \nabla_y k(x,y) 
&= -4 \beta(\beta-1) \left(c^2 + \| \Gamma^{-1/2}(x-y)\|^2\right)^{\beta-2} \| \Gamma^{-1}(x - y)\|^2 \\
&- 2\beta \left(c^2 + \|\Gamma^{-1/2}(x-y)\|^2\right)^{\beta-1} \trace(\Gamma^{-1})
\label{eq:appx:deriv:nablax_nablay}
\end{aligned}
\end{equation}

Substituting (\ref{eq:appx:deviv:nablax}), (\ref{eq:appx:deviv:nablay}) and (\ref{eq:appx:deriv:nablax_nablay}) into (\ref{eq:appx:deriv:stein-kernel}), we obtain
\begin{equation}
\begin{aligned}
k_P(x, y)
= &-4 \beta(\beta-1) \left(c^2 + \| \Gamma^{-1/2}(x-y)\|^2\right)^{\beta-2} \| \Gamma^{-1}(x - y)\|^2 \\
&- 2\beta \left(c^2 + \|\Gamma^{-1/2}(x-y)\|^2\right)^{\beta-1} \trace(\Gamma^{-1}) \\
&+ 2 \beta \left(c^2 + \| \Gamma^{-1/2} (x-y)\|^2\right)^{\beta-1} \langle \Gamma^{-1} (x - y), \nabla_y \log p(y)\rangle \\
&- 2 \beta \left(c^2 + \| \Gamma^{-1/2} (x-y)\|^2\right)^{\beta-1} \langle \Gamma^{-1} (x - y), \nabla_x \log p(x)\rangle \\
&+ \left(c^2 + \| \Gamma^{-1/2} (x-y)\|^2\right)^\beta \langle \nabla_x \log p(x), \nabla_y \log p(y) \rangle \\
= &-4 \beta(\beta-1) D^{\beta-2} \| \Gamma^{-1}(x - y)\|^2  \\
&- 2 \beta D^{\beta-1} (\trace(\Gamma^{-1}) + \langle \Gamma^{-1} (x - y), \nabla_x \log p(x) - \nabla_y \log p(y)\rangle) \\
&+ D^\beta \langle \nabla_x \log p(x), \nabla_y \log p(y) \rangle, \\
\end{aligned}
\end{equation}
where we have denoted $D = c^2 + \| \Gamma^{-1/2}(x-y)\|^2$.


\appendix
\chapter{Code}
\label{appendix:code}



\bibliographystyle{plainnat_modified}
\bibliography{biblio}

\end{document}
