@article{kutnerEconophysicsSociophysicsTheir2019,
  title = {Econophysics and Sociophysics: {{Their}} Milestones \& Challenges},
  shorttitle = {Econophysics and Sociophysics},
  author = {Kutner, Ryszard and Ausloos, Marcel and Grech, Dariusz and Di Matteo, Tiziana and Schinckus, Christophe and Eugene Stanley, H.},
  year = {2019},
  month = feb,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {516},
  pages = {240--253},
  issn = {03784371},
  doi = {10.1016/j.physa.2018.10.019},
  urldate = {2024-01-30},
  langid = {english},
  file = {/home/artem/Zotero/storage/HNZ4PAGJ/Kutner et al. - 2019 - Econophysics and sociophysics Their milestones & .pdf}
}

@article{chakrabortiEconophysicsReviewEmpirical2011,
  title = {Econophysics Review: {{I}}. {{Empirical}} Facts},
  shorttitle = {Econophysics Review},
  author = {Chakraborti, Anirban and Toke, Ioane Muni and Patriarca, Marco and Abergel, Fr{\'e}d{\'e}ric},
  year = {2011},
  month = jul,
  journal = {Quantitative Finance},
  volume = {11},
  number = {7},
  pages = {991--1012},
  issn = {1469-7688, 1469-7696},
  doi = {10.1080/14697688.2010.539248},
  urldate = {2024-01-30},
  langid = {english},
  file = {/home/artem/Zotero/storage/VZQ7PFG7/Chakraborti et al. - 2011 - Econophysics review I. Empirical facts.pdf}
}

@article{chakrabortiEconophysicsReviewII2011,
  title = {Econophysics Review: {{II}}. {{Agent-based}} Models},
  shorttitle = {Econophysics Review},
  author = {Chakraborti, Anirban and Toke, Ioane Muni and Patriarca, Marco and Abergel, Fr{\'e}d{\'e}ric},
  year = {2011},
  month = jul,
  journal = {Quantitative Finance},
  volume = {11},
  number = {7},
  pages = {1013--1041},
  issn = {1469-7688, 1469-7696},
  doi = {10.1080/14697688.2010.539249},
  urldate = {2024-01-30},
  langid = {english},
  file = {/home/artem/Zotero/storage/HYUI267D/Chakraborti et al. - 2011 - Econophysics review II. Agent-based models.pdf}
}

@article{contEmpiricalPropertiesAsset2001,
  title = {Empirical Properties of Asset Returns: Stylized Facts and Statistical Issues},
  shorttitle = {Empirical Properties of Asset Returns},
  author = {Cont, R.},
  year = {2001},
  month = feb,
  journal = {Quantitative Finance},
  volume = {1},
  number = {2},
  pages = {223--236},
  issn = {1469-7688, 1469-7696},
  doi = {10.1080/713665670},
  urldate = {2024-01-30},
  langid = {english}
}

@article{dimatteoExchangesComplexNetworks2003,
  title = {Exchanges in Complex Networks: Income and Wealth Distributions},
  shorttitle = {Exchanges in Complex Networks},
  author = {Di Matteo, T. and Aste, T. and Hyde, S. T.},
  year = {2003},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.COND-MAT/0310544},
  urldate = {2024-01-30},
  abstract = {We investigate the wealth evolution in a system of agents that exchange wealth through a disordered network in presence of an additive stochastic Gaussian noise. We show that the resulting wealth distribution is shaped by the degree distribution of the underlying network and in particular we verify that scale free networks generate distributions with power-law tails in the high-income region. Numerical simulations of wealth exchanges performed on two different kind of networks show the inner relation between the wealth distribution and the network properties and confirm the agreement with a self-consistent solution. We show that empirical data for the income distribution in Australia are qualitatively well described by our theoretical predictions.},
  copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004},
  keywords = {FOS: Economics and business,FOS: Physical sciences,General Finance (q-fin.GN),Statistical Mechanics (cond-mat.stat-mech)}
}

@article{garlaschelliInterplayTopologyDynamics2007,
  title = {Interplay between Topology and Dynamics in the {{World Trade Web}}},
  author = {Garlaschelli, D. and Di Matteo, T. and Aste, T. and Caldarelli, G. and Loffredo, M. I.},
  year = {2007},
  month = may,
  journal = {The European Physical Journal B},
  volume = {57},
  number = {2},
  pages = {159--164},
  issn = {1434-6028, 1434-6036},
  doi = {10.1140/epjb/e2007-00131-6},
  urldate = {2024-01-30},
  langid = {english},
  file = {/home/artem/Zotero/storage/34Z384QL/Garlaschelli et al. - 2007 - Interplay between topology and dynamics in the Wor.pdf}
}

@article{dimatteoInnovationFlowSocial2005,
  title = {Innovation Flow through Social Networks: Productivity Distribution in {{France}} and {{Italy}}},
  shorttitle = {Innovation Flow through Social Networks},
  author = {Di Matteo, T. and Aste, T. and Gallegati, M.},
  year = {2005},
  month = oct,
  journal = {The European Physical Journal B},
  volume = {47},
  number = {3},
  pages = {459--466},
  issn = {1434-6028, 1434-6036},
  doi = {10.1140/epjb/e2005-00332-y},
  urldate = {2024-01-30},
  langid = {english},
  file = {/home/artem/Zotero/storage/5KF62SNG/Di Matteo et al. - 2005 - Innovation flow through social networks productiv.pdf}
}

@article{asteStressTransmissionGranular2001,
  title = {Stress Transmission in Granular Matter},
  author = {Aste, Tomaso and Di Matteo, Tiziana and {d'Agliano}, Enrico Galleani},
  year = {2001},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.COND-MAT/0112311},
  urldate = {2024-01-30},
  abstract = {The transmission of forces through a disordered granular system is studied by means of a geometrical-topological approach that reduces the granular packing into a set of layers. This layered structure constitutes the skeleton through which the force chains set up. Given the granular packing, and the region where the force is applied, such a skeleton is uniquely defined. Within this framework, we write an equation for the transmission of the vertical forces that can be solved recursively layer by layer. We find that a special class of analytical solutions for this equation are L{\'e}vi-stable distributions. We discuss the link between criticality and fragility and we show how the disordered packing naturally induces the formation of force-chains and arches. We point out that critical regimes, with power law distributions, are associated with the roughness of the topological layers. Whereas, fragility is associated with local changes in the force network induced by local granular rearrangements or by changes in the applied force. The results are compared with recent experimental observations in particulate matter and with computer simulations.},
  copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004},
  keywords = {Disordered Systems and Neural Networks (cond-mat.dis-nn),FOS: Physical sciences,Soft Condensed Matter (cond-mat.soft)}
}

@article{clementiPowerlawTailExponent2006,
  title = {The Power-Law Tail Exponent of Income Distributions},
  author = {Clementi, F. and Di Matteo, T. and Gallegati, M.},
  year = {2006},
  month = oct,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {370},
  number = {1},
  pages = {49--53},
  issn = {03784371},
  doi = {10.1016/j.physa.2006.04.027},
  urldate = {2024-01-30},
  langid = {english},
  file = {/home/artem/Zotero/storage/KJPXXVXN/Clementi et al. - 2006 - The power-law tail exponent of income distribution.pdf}
}

@article{banerjeeStudyPersonalIncome2006,
  title = {A Study of the Personal Income Distribution in {{Australia}}},
  author = {Banerjee, Anand and Yakovenko, Victor M. and Di Matteo, T.},
  year = {2006},
  month = oct,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {370},
  number = {1},
  pages = {54--59},
  issn = {03784371},
  doi = {10.1016/j.physa.2006.04.023},
  urldate = {2024-01-30},
  langid = {english},
  file = {/home/artem/Zotero/storage/WFDTJYFN/Banerjee et al. - 2006 - A study of the personal income distribution in Aus.pdf}
}

@article{clementiGeneralizedDistributionNew2008,
  title = {The -Generalized Distribution: {{A}} New Descriptive Model for the Size Distribution of Incomes},
  shorttitle = {The -Generalized Distribution},
  author = {Clementi, F. and Di Matteo, T. and Gallegati, M. and Kaniadakis, G.},
  year = {2008},
  month = may,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {387},
  number = {13},
  pages = {3201--3208},
  issn = {03784371},
  doi = {10.1016/j.physa.2008.01.109},
  urldate = {2024-01-30},
  langid = {english},
  file = {/home/artem/Zotero/storage/GKDT3HWJ/Clementi et al. - 2008 - The -generalized distribution A new descriptive m.pdf}
}

@article{stephensDealingLabelSwitching2000,
  title = {Dealing {{With Label Switching}} in {{Mixture Models}}},
  author = {Stephens, Matthew},
  year = {2000},
  month = nov,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {62},
  number = {4},
  pages = {795--809},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/1467-9868.00265},
  urldate = {2024-01-30},
  abstract = {Summary             In a Bayesian analysis of finite mixture models, parameter estimation and clustering are sometimes less straightforward than might be expected. In particular, the common practice of estimating parameters by their posterior mean, and summarizing joint posterior distributions by marginal distributions, often leads to nonsensical answers. This is due to the so-called `label switching' problem, which is caused by symmetry in the likelihood of the model parameters. A frequent response to this problem is to remove the symmetry by using artificial identifiability constraints. We demonstrate that this fails in general to solve the problem, and we describe an alternative class of approaches, relabelling algorithms, which arise from attempting to minimize the posterior expected loss under a class of loss functions. We describe in detail one particularly simple and general relabelling algorithm and illustrate its success in dealing with the label switching problem on two examples.},
  langid = {english},
  file = {/home/artem/Zotero/storage/BUWR2MMI/Stephens - 2000 - Dealing With Label Switching in Mixture Models.pdf}
}

@article{jasraMarkovChainMonte2005,
  title = {Markov {{Chain Monte Carlo Methods}} and the {{Label Switching Problem}} in {{Bayesian Mixture Modeling}}},
  author = {Jasra, A. and Holmes, C. C. and Stephens, D. A.},
  year = {2005},
  month = feb,
  journal = {Statistical Science},
  volume = {20},
  number = {1},
  issn = {0883-4237},
  doi = {10.1214/088342305000000016},
  urldate = {2024-01-30},
  file = {/home/artem/Zotero/storage/8NEE6K6U/Jasra et al. - 2005 - Markov Chain Monte Carlo Methods and the Label Swi.pdf}
}

@article{mantegnaHierarchicalStructureFinancial1999,
  title = {Hierarchical Structure in Financial Markets},
  author = {Mantegna, R.N.},
  year = {1999},
  month = sep,
  journal = {The European Physical Journal B},
  volume = {11},
  number = {1},
  pages = {193--197},
  issn = {1434-6028},
  doi = {10.1007/s100510050929},
  urldate = {2024-02-05},
  langid = {english},
  file = {/home/artem/Zotero/storage/8J4GZZR3/Mantegna - 1999 - Hierarchical structure in financial markets.pdf}
}

@article{mantegnaScalingBehaviourDynamics1995,
  title = {Scaling Behaviour in the Dynamics of an Economic Index},
  author = {Mantegna, Rosario N. and Stanley, H. Eugene},
  year = {1995},
  month = jul,
  journal = {Nature},
  volume = {376},
  number = {6535},
  pages = {46--49},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/376046a0},
  urldate = {2024-02-05},
  langid = {english}
}

@article{nguyenResidualReturnReversals2019,
  title = {Residual Return Reversals: {{European}} Evidence},
  shorttitle = {Residual Return Reversals},
  author = {Nguyen, Anh Duy},
  year = {2019},
  month = dec,
  journal = {Research in International Business and Finance},
  volume = {50},
  pages = {392--397},
  issn = {02755319},
  doi = {10.1016/j.ribaf.2019.06.011},
  urldate = {2024-02-05},
  langid = {english},
  file = {/home/artem/Zotero/storage/AA8PFGGZ/Nguyen - 2019 - Residual return reversals European evidence.pdf}
}

@misc{harchol-balterApplyingPhPrograms,
  title = {Applying to {{Ph}}.{{D}}. {{Programs}} in {{Computer Science}}},
  author = {{Harchol-Balter}, Mor},
  file = {/home/artem/Zotero/storage/KS654WW5/Harchol-Balter - Applying to Ph.D. Programs in Computer Science.pdf}
}

@article{vermaNewSetCluster2020,
  title = {A New Set of Cluster Driven Composite Development Indicators},
  author = {Verma, Anshul and Angelini, Orazio and Di Matteo, Tiziana},
  year = {2020},
  month = dec,
  journal = {EPJ Data Science},
  volume = {9},
  number = {1},
  pages = {8},
  issn = {2193-1127},
  doi = {10.1140/epjds/s13688-020-00225-y},
  urldate = {2024-04-08},
  abstract = {Abstract             Composite development indicators used in policy making often subjectively aggregate a restricted set of indicators. We show, using dimensionality reduction techniques, including Principal Component Analysis (PCA) and for the first time information filtering and hierarchical clustering, that these composite indicators miss key information on the relationship between different indicators. In particular, the grouping of indicators via topics is not reflected in the data at a global and local level. We overcome these issues by using the clustering of indicators to build a new set of cluster driven composite development indicators that are objective, data driven, comparable between countries, and retain interpretabilty. We discuss their consequences on informing policy makers about country development, comparing them with the top PageRank indicators as a benchmark. Finally, we demonstrate that our new set of composite development indicators outperforms the benchmark on a dataset reconstruction task.},
  langid = {english},
  file = {/home/artem/Zotero/storage/M9IE2ZNY/Verma et al. - 2020 - A new set of cluster driven composite development .pdf}
}

@article{tumminelloToolFilteringInformation2005,
  title = {A Tool for Filtering Information in Complex Systems},
  author = {Tumminello, M. and Aste, T. and Di Matteo, T. and Mantegna, R. N.},
  year = {2005},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {102},
  number = {30},
  pages = {10421--10426},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0500298102},
  urldate = {2024-04-09},
  abstract = {We introduce a technique to filter out complex data sets by extracting a subgraph of representative links. Such a filtering can be tuned up to any desired level by controlling the genus of the resulting graph. We show that this technique is especially suitable for correlation-based graphs, giving filtered graphs that preserve the hierarchical organization of the minimum spanning tree but containing a larger amount of information in their internal structure. In particular in the case of planar filtered graphs (genus equal to 0), triangular loops and four-element cliques are formed. The application of this filtering procedure to 100 stocks in the U.S. equity markets shows that such loops and cliques have important and significant relationships with the market structure and properties.},
  langid = {english},
  file = {/home/artem/Zotero/storage/IWT5BW9P/Tumminello et al. - 2005 - A tool for filtering information in complex system.pdf}
}

@article{vehtariRankNormalizationFoldingLocalization2021,
  title = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}: {{An Improved R}} for {{Assessing Convergence}} of {{MCMC}} (with {{Discussion}})},
  shorttitle = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  year = {2021},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {16},
  number = {2},
  pages = {667--718},
  issn = {1936-0975},
  doi = {10.1214/20-BA1221},
  urldate = {2024-06-24},
  abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic R of Gelman and Rubin (1992) has serious flaws. Traditional R will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice.},
  langid = {english}
}

@article{hobertHonestExplorationIntractable2001,
  title = {Honest {{Exploration}} of {{Intractable Probability Distributions}} via {{Markov Chain Monte Carlo}}},
  author = {Hobert, James P. and Jones, Galin L.},
  year = {2001},
  month = nov,
  journal = {Statistical Science},
  volume = {16},
  number = {4},
  issn = {0883-4237},
  doi = {10.1214/ss/1015346317},
  urldate = {2024-06-24},
  abstract = {Two important questions that must be answered whenever a Markov chain Monte Carlo (MCMC) algorithm is used are (Q1) What is an appropriate burn-in? and (Q2) How long should the sampling continue after burn-in? Developing rigorous answers to these questions presently requires a detailed study of the convergence properties of the underlying Markov chain. Consequently, in most practical applications of MCMC, exact answers to (Q1) and (Q2) are not sought. The goal of this paper is to demystify the analysis that leads to honest answers to (Q1) and (Q2). The authors hope that this article will serve as a bridge between those developing Markov chain theory and practitioners using MCMC to solve practical problems.},
  langid = {english},
  file = {/home/artem/Zotero/storage/QPVKYRA5/Hobert and Jones - 2001 - Honest Exploration of Intractable Probability Dist.pdf}
}

@article{hastingsMonteCarloSampling1970,
  title = {Monte {{Carlo}} Sampling Methods Using {{Markov}} Chains and Their Applications},
  author = {Hastings, Wilfred Keith},
  year = {1970},
  month = apr,
  journal = {Biometrika},
  volume = {57},
  number = {1},
  pages = {97--109},
  doi = {10.2307/2334940},
  abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
  langid = {english},
  file = {/home/artem/Zotero/storage/JHRFH27B/Hastings - Monte Carlo sampling methods using Markov chains a.pdf}
}

@article{metropolisEquationStateCalculations1953,
  title = {Equation of {{State Calculations}} by {{Fast Computing Machines}}},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
  year = {1953},
  month = jun,
  journal = {The Journal of Chemical Physics},
  volume = {21},
  number = {6},
  pages = {1087--1092},
  issn = {0021-9606, 1089-7690},
  doi = {10.1063/1.1699114},
  urldate = {2024-06-24},
  abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two-dimensional rigid-sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four-term virial coefficient expansion.},
  langid = {english},
  file = {/home/artem/Zotero/storage/7Y2Z4C22/Metropolis et al. - 1953 - Equation of State Calculations by Fast Computing M.pdf}
}

@misc{anastasiouSteinMethodMeets2022,
  title = {Stein's {{Method Meets Computational Statistics}}: {{A Review}} of {{Some Recent Developments}}},
  shorttitle = {Stein's {{Method Meets Computational Statistics}}},
  author = {Anastasiou, Andreas and Barp, Alessandro and Briol, Fran{\c c}ois-Xavier and Ebner, Bruno and Gaunt, Robert E. and Ghaderinezhad, Fatemeh and Gorham, Jackson and Gretton, Arthur and Ley, Christophe and Liu, Qiang and Mackey, Lester and Oates, Chris J. and Reinert, Gesine and Swan, Yvik},
  year = {2022},
  month = jun,
  number = {arXiv:2105.03481},
  eprint = {2105.03481},
  primaryclass = {math, stat},
  publisher = {arXiv},
  urldate = {2024-06-24},
  abstract = {Stein's method compares probability distributions through the study of a class of linear operators called Stein operators. While mainly studied in probability and used to underpin theoretical statistics, Stein's method has led to significant advances in computational statistics in recent years. The goal of this survey is to bring together some of these recent developments and, in doing so, to stimulate further research into the successful field of Stein's method and statistics. The topics we discuss include tools to benchmark and compare sampling methods such as approximate Markov chain Monte Carlo, deterministic alternatives to sampling methods, control variate techniques, parameter estimation and goodness-of-fit testing.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Computation,Statistics - Methodology},
  file = {/home/artem/Zotero/storage/N3QU755P/Anastasiou et al. - 2022 - Stein's Method Meets Computational Statistics A R.pdf}
}

@article{chopinFastCompressionMCMC2021,
  title = {Fast Compression of {{MCMC}} Output},
  author = {Chopin, Nicolas and Ducrocq, Gabriel},
  year = {2021},
  month = aug,
  journal = {Entropy},
  volume = {23},
  number = {8},
  eprint = {2107.04552},
  primaryclass = {stat},
  pages = {1017},
  issn = {1099-4300},
  doi = {10.3390/e23081017},
  urldate = {2024-06-24},
  abstract = {We propose cube thinning, a novel method for compressing the output of a MCMC (Markov chain Monte Carlo) algorithm when control variates are available. It amounts to resampling the initial MCMC sample (according to weights derived from control variates), while imposing equality constraints on averages of these control variates, using the cube method of {\textbackslash}cite\{Deville2004\}. Its main advantage is that its CPU cost is linear in \$N\$, the original sample size, and is constant in \$M\$, the required size for the compressed sample. This compares favourably to Stein thinning {\textbackslash}citep\{Riabiz2020\}, which has complexity \${\textbackslash}mathcal\{O\}(NM{\textasciicircum}2)\$, and which requires the availability of the gradient of the target log-density (which automatically implies the availability of control variates). Our numerical experiments suggest that cube thinning is also competitive in terms of statistical error.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/artem/Zotero/storage/37VCFWF4/Chopin and Ducrocq - 2021 - Fast compression of MCMC output.pdf}
}

@misc{hawkinsOnlineInformativeMCMC2022,
  title = {Online, {{Informative MCMC Thinning}} with {{Kernelized Stein Discrepancy}}},
  author = {Hawkins, Cole and Koppel, Alec and Zhang, Zheng},
  year = {2022},
  month = apr,
  number = {arXiv:2201.07130},
  eprint = {2201.07130},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-06-24},
  abstract = {A fundamental challenge in Bayesian inference is efficient representation of a target distribution. Many non-parametric approaches do so by sampling a large number of points using variants of Markov Chain Monte Carlo (MCMC). We propose an MCMC variant that retains only those posterior samples which exceed a KSD threshold, which we call KSD Thinning. We establish the convergence and complexity tradeoffs for several settings of KSD Thinning as a function of the KSD threshold parameter, sample size, and other problem parameters. Finally, we provide experimental comparisons against other online nonparametric Bayesian methods that generate low-complexity posterior representations, and observe superior consistency/complexity tradeoffs. Our attached code will be made public.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/artem/Zotero/storage/MH5N65FN/Hawkins et al. - 2022 - Online, Informative MCMC Thinning with Kernelized .pdf}
}

@inproceedings{fisherGradientFreeKernelStein2024,
  title = {Gradient-{{Free Kernel Stein Discrepancy}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Fisher, Matthew A. and Oates, Chris J.},
  year = {2024},
  month = may,
  eprint = {2207.02636},
  primaryclass = {math, stat},
  pages = {23855--23885},
  urldate = {2024-06-24},
  abstract = {Stein discrepancies have emerged as a powerful statistical tool, being applied to fundamental statistical problems including parameter inference, goodness-of-fit testing, and sampling. The canonical Stein discrepancies require the derivatives of a statistical model to be computed, and in return provide theoretical guarantees of convergence detection and control. However, for complex statistical models, the stable numerical computation of derivatives can require bespoke algorithmic development and render Stein discrepancies impractical. This paper focuses on posterior approximation using Stein discrepancies, and introduces a collection of non-canonical Stein discrepancies that are gradient-free, meaning that derivatives of the statistical model are not required. Sufficient conditions for convergence detection and control are established, and applications to sampling and variational inference are presented.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Computation},
  file = {/home/artem/Zotero/storage/I3IR4XQ5/Fisher and Oates - 2022 - Gradient-Free Kernel Stein Discrepancy.pdf}
}

@misc{kanagawaControllingMomentsKernel2024,
  title = {Controlling {{Moments}} with {{Kernel Stein Discrepancies}}},
  author = {Kanagawa, Heishiro and Barp, Alessandro and Gretton, Arthur and Mackey, Lester},
  year = {2024},
  month = jan,
  number = {arXiv:2211.05408},
  eprint = {2211.05408},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-06-24},
  abstract = {Kernel Stein discrepancies (KSDs) measure the quality of a distributional approximation and can be computed even when the target density has an intractable normalizing constant. Notable applications include the diagnosis of approximate MCMC samplers and goodness-of-fit tests for unnormalized statistical models. The present work analyzes the convergence control properties of KSDs. We first show that standard KSDs used for weak convergence control fail to control moment convergence. To address this limitation, we next provide sufficient conditions under which alternative diffusion KSDs control both moment and weak convergence. As an immediate consequence we develop, for each q {$>$} 0, the first KSDs known to exactly characterize q-Wasserstein convergence.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/artem/Zotero/storage/4X7D2FM9/Kanagawa et al. - 2024 - Controlling Moments with Kernel Stein Discrepancie.pdf}
}

@misc{benardKernelSteinDiscrepancy2023,
  title = {Kernel {{Stein Discrepancy}} Thinning: A Theoretical Perspective of Pathologies and a Practical Fix with Regularization},
  shorttitle = {Kernel {{Stein Discrepancy}} Thinning},
  author = {B{\'e}nard, Cl{\'e}ment and Staber, Brian and Da Veiga, S{\'e}bastien},
  year = {2023},
  month = oct,
  number = {arXiv:2301.13528},
  eprint = {2301.13528},
  primaryclass = {math, stat},
  publisher = {arXiv},
  urldate = {2024-06-24},
  abstract = {Stein thinning is a promising algorithm proposed by Riabiz et al. [2022] for postprocessing outputs of Markov chain Monte Carlo (MCMC). The main principle is to greedily minimize the kernelized Stein discrepancy (KSD), which only requires the gradient of the log-target distribution, and is thus well-suited for Bayesian inference. The main advantages of Stein thinning are the automatic remove of the burn-in period, the correction of the bias introduced by recent MCMC algorithms, and the asymptotic properties of convergence towards the target distribution. Nevertheless, Stein thinning suffers from several empirical pathologies, which may result in poor approximations, as observed in the literature. In this article, we conduct a theoretical analysis of these pathologies, to clearly identify the mechanisms at stake, and suggest improved strategies. Then, we introduce the regularized Stein thinning algorithm to alleviate the identified pathologies. Finally, theoretical guarantees and extensive experiments show the high efficiency of the proposed algorithm. An implementation of regularized Stein thinning as the kernax library in python and JAX is available at https://gitlab.com/drti/kernax.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/artem/Zotero/storage/XQHK48NI/Bénard et al. - 2023 - Kernel Stein Discrepancy thinning a theoretical p.pdf}
}

@misc{huangEnhancingSampleQuality2023,
  title = {Enhancing {{Sample Quality}} through {{Minimum Energy Importance Weights}}},
  author = {Huang, Chaofan and Joseph, V. Roshan},
  year = {2023},
  month = dec,
  number = {arXiv:2310.07953},
  eprint = {2310.07953},
  primaryclass = {stat},
  publisher = {arXiv},
  urldate = {2024-06-24},
  abstract = {Importance sampling is a powerful tool for correcting the distributional mismatch in many statistical and machine learning problems, but in practice its performance is limited by the usage of simple proposals whose importance weights can be computed analytically. To address this limitation, Liu and Lee (2017) proposed a Black-Box Importance Sampling (BBIS) algorithm that computes the importance weights for arbitrary simulated samples by minimizing the kernelized Stein discrepancy. However, this requires knowing the score function of the target distribution, which is not easy to compute for many Bayesian problems. Hence, in this paper we propose another novel BBIS algorithm using minimum energy design, BBIS-MED, that requires only the unnormalized density function, which can be utilized as a post-processing step to improve the quality of Markov Chain Monte Carlo samples. We demonstrate the effectiveness and wide applicability of our proposed BBIS-MED algorithm on extensive simulations and a real-world Bayesian model calibration problem where the score function cannot be derived analytically.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/artem/Zotero/storage/ZEUSN696/Huang and Joseph - 2023 - Enhancing Sample Quality through Minimum Energy Im.pdf}
}

@article{hobertHonestExplorationIntractable2001a,
  title = {Honest {{Exploration}} of {{Intractable Probability Distributions}} via {{Markov Chain Monte Carlo}}},
  author = {Hobert, James P. and Jones, Galin L.},
  year = {2001},
  month = nov,
  journal = {Statistical Science},
  volume = {16},
  number = {4},
  issn = {0883-4237},
  doi = {10.1214/ss/1015346317},
  urldate = {2024-06-24},
  abstract = {Two important questions that must be answered whenever a Markov chain Monte Carlo (MCMC) algorithm is used are (Q1) What is an appropriate burn-in? and (Q2) How long should the sampling continue after burn-in? Developing rigorous answers to these questions presently requires a detailed study of the convergence properties of the underlying Markov chain. Consequently, in most practical applications of MCMC, exact answers to (Q1) and (Q2) are not sought. The goal of this paper is to demystify the analysis that leads to honest answers to (Q1) and (Q2). The authors hope that this article will serve as a bridge between those developing Markov chain theory and practitioners using MCMC to solve practical problems.},
  langid = {english},
  file = {/home/artem/Zotero/storage/4TNWB6HC/Hobert and Jones - 2001 - Honest Exploration of Intractable Probability Dist.pdf}
}

@article{robertsOptimalScalingVarious2001,
  title = {Optimal Scaling for Various {{Metropolis-Hastings}} Algorithms},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  year = {2001},
  month = nov,
  journal = {Statistical Science},
  volume = {16},
  number = {4},
  pages = {351--367},
  issn = {0883-4237},
  doi = {10.1214/ss/1015346320},
  urldate = {2024-06-24},
  abstract = {We review and extend results related to optimal scaling of Metropolis--Hastings algorithms. We present various theoretical results for the high-dimensional limit. We also present simulation studies which confirm the theoretical results in finite-dimensional contexts.},
  langid = {english},
  file = {/home/artem/Zotero/storage/S5Y857PE/Roberts and Rosenthal - 2001 - Optimal scaling for various Metropolis-Hastings al.pdf}
}

@article{gelmanWeakConvergenceOptimal1997,
  title = {Weak Convergence and Optimal Scaling of Random Walk {{Metropolis}} Algorithms},
  author = {Gelman, Andrew and Gilks, Walter R. and Roberts, Gareth O.},
  year = {1997},
  month = feb,
  journal = {The Annals of Applied Probability},
  volume = {7},
  number = {1},
  pages = {110--120},
  issn = {1050-5164},
  doi = {10.1214/aoap/1034625254},
  urldate = {2024-06-24},
  langid = {english},
  file = {/home/artem/Zotero/storage/U4JPHWZG/Gelman et al. - 1997 - Weak convergence and optimal scaling of random wal.pdf}
}

@article{brooksGeneralMethodsMonitoring1998,
  title = {General {{Methods}} for {{Monitoring Convergence}} of {{Iterative Simulations}}},
  author = {Brooks, Stephen P. and Gelman, Andrew},
  year = {1998},
  month = dec,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {7},
  number = {4},
  pages = {434--455},
  issn = {1061-8600, 1537-2715},
  doi = {10.1080/10618600.1998.10474787},
  urldate = {2024-06-24},
  langid = {english},
  file = {/home/artem/Zotero/storage/KVRDBT7I/Brooks and Gelman - 1998 - General Methods for Monitoring Convergence of Iter.pdf}
}

@article{nuskenKernelMethodsMachine,
  title = {Kernel {{Methods}} in {{Machine Learning}} and {{Statistics}}},
  author = {Nusken, Nikolas and Riabiz, Marina},
  langid = {english},
  file = {/home/artem/Zotero/storage/RV5FWNA6/Nusken and Riabiz - Kernel Methods in Machine Learning and Statistics.pdf}
}

@article{riabizOptimalThinningMCMC2022,
  title = {Optimal {{Thinning}} of {{MCMC Output}}},
  author = {Riabiz, Marina and Chen, Wilson Ye and Cockayne, Jon and Swietach, Pawel and Niederer, Steven A. and Mackey, Lester and Oates, {\relax Chris}. J.},
  year = {2022},
  month = sep,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {84},
  number = {4},
  pages = {1059--1081},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/rssb.12503},
  urldate = {2024-06-24},
  abstract = {The use of heuristics to assess the convergence and compress the output of Markov chain Monte Carlo can be sub-optimal in terms of the empirical approximations that are produced. Typically a number of the initial states are attributed to `burn in' and removed, while the remainder of the chain is `thinned' if compression is also required. In this paper, we consider the problem of retrospectively selecting a subset of states, of fixed cardinality, from the sample path such that the approximation provided by their empirical distribution is close to optimal. A novel method is proposed, based on greedy minimisation of a kernel Stein discrepancy, that is suitable when the gradient of the log-target can be evaluated and approximation using a small number of states is required. Theoretical results guarantee consistency of the method and its effectiveness is demonstrated in the challenging context of parameter inference for ordinary differential equations. Software is available in the Stein Thinning package in Python, R and MATLAB.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/artem/Zotero/storage/5UUJIBY3/Riabiz et al. - 2022 - Optimal Thinning of MCMC Output.pdf;/home/artem/Zotero/storage/R6DEKIDR/Supplementary Material.pdf}
}

@article{gelmanInferenceIterativeSimulation1992,
  title = {Inference from {{Iterative Simulation Using Multiple Sequences}}},
  author = {Gelman, Andrew and Rubin, Donald B.},
  year = {1992},
  month = nov,
  journal = {Statistical Science},
  volume = {7},
  number = {4},
  pages = {457--472},
  issn = {0883-4237},
  doi = {10.1214/ss/1177011136},
  urldate = {2024-06-24},
  file = {/home/artem/Zotero/storage/AJV7CJ58/Gelman and Rubin - 1992 - Inference from Iterative Simulation Using Multiple.pdf}
}

@incollection{gelmanEfficientMetropolisJumping1996,
  title = {Efficient {{Metropolis Jumping Rules}}},
  booktitle = {Bayesian {{Statistics}}},
  author = {Gelman, Andrew and Roberts, Gareth O. and Gilks, Walter R.},
  year = {1996},
  month = may,
  volume = {5},
  pages = {599--608},
  publisher = {Oxford University Press},
  address = {Oxford},
  doi = {10.1093/oso/9780198523567.003.0038},
  urldate = {2024-06-27},
  abstract = {Abstract             The algorithm of Metropolis et al. (1953) and its generalizations have been increasingly popular in computational physics and, more recently, statistics, for sampling from intractable multivariate distributions. Much recent research has been devoted to increasing the efficiency of simulation algorithms by altering the jumping rules for Metropolis-like algorithms. We study a very specific question: What are the most efficient symmetric jumping kernels for simulating a normal target distribution using the Metropolis algorithm{\~a} We provide a general theoretical result as the dimension of a class of canonical problems goes to {$\infty$} and numerical approximations and simulations for low-dimensional Gaussian target distributions that show that the limiting results provide extremely accurate approximations in six and higher dimensions.},
  isbn = {978-0-19-852356-7 978-1-383-02394-7},
  langid = {english},
  file = {/home/artem/Zotero/storage/5S79BN3J/efficient metropolis jumping rules.pdf}
}

@article{vatsRevisitingGelmanRubin2021,
  title = {Revisiting the {{Gelman}}--{{Rubin Diagnostic}}},
  author = {Vats, Dootika and Knudson, Christina},
  year = {2021},
  month = nov,
  journal = {Statistical Science},
  volume = {36},
  number = {4},
  pages = {518--529},
  issn = {0883-4237},
  doi = {10.1214/20-STS812},
  urldate = {2024-06-27},
  file = {/home/artem/Zotero/storage/5QK4AQN4/Vats and Knudson - 2021 - Revisiting the Gelman–Rubin Diagnostic.pdf}
}

@misc{oatesMinimumKernelDiscrepancy2022,
  title = {Minimum {{Kernel Discrepancy Estimators}}},
  author = {Oates, {\relax Chris}. J.},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2210.16357},
  urldate = {2024-07-06},
  abstract = {For two decades, reproducing kernels and their associated discrepancies have facilitated elegant theoretical analyses in the setting of quasi Monte Carlo. These same tools are now receiving interest in statistics and related fields, as criteria that can be used to select an appropriate statistical model for a given dataset. The focus of this article is on minimum kernel discrepancy estimators, whose use in statistical applications is reviewed, and a general theoretical framework for establishing their asymptotic properties is presented.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {FOS: Computer and information sciences,FOS: Mathematics,Methodology (stat.ME),Statistics Theory (math.ST)}
}

@article{cramerCompositionElementaryErrors1928,
  title = {On the Composition of Elementary Errors: {{First}} Paper: {{Mathematical}} Deductions},
  shorttitle = {On the Composition of Elementary Errors},
  author = {Cram{\'e}r, Harald},
  year = {1928},
  month = jan,
  journal = {Scandinavian Actuarial Journal},
  volume = {1928},
  number = {1},
  pages = {13--74},
  issn = {0346-1238, 1651-2030},
  doi = {10.1080/03461238.1928.10416862},
  urldate = {2024-07-11},
  langid = {english},
  file = {/home/artem/Zotero/storage/VDJBU7HB/Cramér - 1928 - On the composition of elementary errors First pap.pdf}
}

@article{cramerCompositionElementaryErrors1928a,
  title = {On the Composition of Elementary Errors: {{Second}} Paper: {{Statistical}} Applications},
  shorttitle = {On the Composition of Elementary Errors},
  author = {Cram{\'e}r, Harald},
  year = {1928},
  month = jan,
  journal = {Scandinavian Actuarial Journal},
  volume = {1928},
  number = {1},
  pages = {141--180},
  issn = {0346-1238, 1651-2030},
  doi = {10.1080/03461238.1928.10416872},
  urldate = {2024-07-11},
  langid = {english},
  file = {/home/artem/Zotero/storage/MZPR5T5V/Cramér - 1928 - On the composition of elementary errors Second pa.pdf}
}

@article{baringhausNewMultivariateTwosample2004,
  title = {On a New Multivariate Two-Sample Test},
  author = {Baringhaus, Ludwig and Franz, C.},
  year = {2004},
  month = jan,
  journal = {Journal of Multivariate Analysis},
  volume = {88},
  number = {1},
  pages = {190--206},
  issn = {0047259X},
  doi = {10.1016/S0047-259X(03)00079-4},
  urldate = {2024-07-11},
  abstract = {In this paper we propose a new test for the multivariate two-sample problem. The test statistic is the difference of the sum of all the Euclidean interpoint distances between the random variables from the two different samples and one-half of the two corresponding sums of distances of the variables within the same sample. The asymptotic null distribution of the test statistic is derived using the projection method and shown to be the limit of the bootstrap distribution. A simulation study includes the comparison of univariate and multivariate normal distributions for location and dispersion alternatives. For normal location alternatives the new test is shown to have power similar to that of the t- and T2-Test.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/artem/Zotero/storage/QM4IFGTE/Baringhaus and Franz - 2004 - On a new multivariate two-sample test.pdf}
}

@article{rizzoEnergyDistance2016,
  title = {Energy {{Distance}}},
  author = {Rizzo, Maria L. and Sz{\'e}kely, G{\'a}bor J.},
  year = {2016},
  month = jan,
  journal = {WIREs Computational Statistics},
  volume = {8},
  number = {1},
  pages = {27--38},
  issn = {1939-5108, 1939-0068},
  doi = {10.1002/wics.1375},
  urldate = {2024-07-11},
  abstract = {Energy distance is a metric that measures the distance between the distributions of random vectors. Energy distance is zero if and only if the distributions are identical, thus it characterizes equality of distributions and provides a theoretical foundation for statistical inference and analysis. Energy statistics are functions of distances between observations in metric spaces. As a statistic, energy distance can be applied to measure the difference between a sample and a hypothesized distribution or the difference between two or more samples in arbitrary, not necessarily equal dimensions. The name energy is inspired by the close analogy with Newton's gravitational potential energy. Applications include testing independence by distance covariance, goodness-of-fit, nonparametric tests for equality of distributions and extension of analysis of variance, generalizations of clustering algorithms, change point analysis, feature selection, and more.               WIREs Comput Stat               2016, 8:27--38. doi: 10.1002/wics.1375                                         This article is categorized under:                                                   Statistical and Graphical Methods of Data Analysis {$>$} Multivariate Analysis                                                     Statistical and Graphical Methods of Data Analysis {$>$} Nonparametric Methods},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {/home/artem/Zotero/storage/LC6LFS26/Rizzo and Székely - 2016 - Energy distance.pdf}
}

@inproceedings{gorhamMeasuringSampleQuality2015,
  title = {Measuring {{Sample Quality}} with {{Stein}}'s {{Method}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gorham, Jackson and Mackey, Lester},
  year = {2015},
  volume = {28},
  eprint = {1506.03039},
  primaryclass = {cs, math, stat},
  pages = {226--234},
  publisher = {MIT Press},
  urldate = {2024-08-19},
  abstract = {To improve the efficiency of Monte Carlo estimation, practitioners are turning to biased Markov chain Monte Carlo procedures that trade off asymptotic exactness for computational speed. The reasoning is sound: a reduction in variance due to more rapid sampling can outweigh the bias introduced. However, the inexactness creates new challenges for sampler and parameter selection, since standard measures of sample quality like effective sample size do not account for asymptotic bias. To address these challenges, we introduce a new computable quality measure based on Stein's method that quantifies the maximum discrepancy between sample and target expectations over a large class of test functions. We use our tool to compare exact, biased, and deterministic sample sequences and illustrate applications to hyperparameter selection, convergence rate assessment, and quantifying bias-variance tradeoffs in posterior inference.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Probability,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/artem/Zotero/storage/4DAEGEP9/Gorham and Mackey - 2018 - Measuring Sample Quality with Stein's Method.pdf}
}

@inproceedings{steinBoundErrorNormal1972,
  title = {A {{Bound}} for the {{Error}} in the {{Normal Approximation}} to the {{Distribution}} of a {{Sum}} of {{Dependent Random Variables}}},
  booktitle = {Proceedings of the 6th {{Berkeley Symposium}} on {{Mathematical Statistics}} and {{Probability}}},
  author = {Stein, Charles},
  year = {1972},
  pages = {583--602},
  file = {/home/artem/Zotero/storage/HQ2FEEFB/1200514239.pdf}
}

@article{mullerIntegralProbabilityMetrics1997,
  title = {Integral {{Probability Metrics}} and {{Their Generating Classes}} of {{Functions}}},
  author = {M{\"u}ller, Alfred},
  year = {1997},
  month = jun,
  journal = {Advances in Applied Probability},
  volume = {29},
  number = {2},
  pages = {429--443},
  issn = {0001-8678, 1475-6064},
  doi = {10.2307/1428011},
  urldate = {2024-08-19},
  abstract = {We consider probability metrics of the following type: for a class                                of functions and probability measures                P, Q                we define                                A unified study of such                integral probability metrics                is given. We characterize the maximal class of functions that generates such a metric. Further, we show how some interesting properties of these probability metrics arise directly from conditions on the generating class of functions. The results are illustrated by several examples, including the Kolmogorov metric, the Dudley metric and the stop-loss metric.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  file = {/home/artem/Zotero/storage/FBG2NPJ8/Mller-IntegralProbabilityMetrics-1997.pdf}
}

@article{kearnsEmpiricalLimitationsHighFrequency2010,
  title = {Empirical {{Limitations}} on {{High-Frequency Trading Profitability}}},
  author = {Kearns, Michael and Kulesza, Alex and Nevmyvaka, Yuriy},
  year = {2010},
  month = sep,
  journal = {The Journal of Trading},
  volume = {5},
  number = {4},
  pages = {50--62},
  issn = {1559-3967, 2168-8427},
  doi = {10.3905/jot.2010.5.4.050},
  urldate = {2024-08-19},
  langid = {english}
}

@article{almgrenDirectEstimationEquity2005,
  title = {Direct {{Estimation}} of {{Equity Market Impact}}},
  author = {Almgren, Robert and Thum, Chee and Hauptmann, Emmanuel and Li, Hong},
  year = {2005},
  file = {/home/artem/Zotero/storage/6QI4CZZW/costestim.pdf}
}

@inproceedings{gorhamMeasuringSampleQuality2017,
  title = {Measuring {{Sample Quality}} with {{Kernels}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Gorham, Jackson and Mackey, Lester},
  year = {2017},
  pages = {1292--1301},
  publisher = {PLMR},
  address = {Sydney, Australia},
  doi = {10.48550/ARXIV.1703.01717},
  urldate = {2024-08-19},
  abstract = {Approximate Markov chain Monte Carlo (MCMC) offers the promise of more rapid sampling at the cost of more biased inference. Since standard MCMC diagnostics fail to detect these biases, researchers have developed computable Stein discrepancy measures that provably determine the convergence of a sample to its target distribution. This approach was recently combined with the theory of reproducing kernels to define a closed-form kernel Stein discrepancy (KSD) computable by summing kernel evaluations across pairs of sample points. We develop a theory of weak convergence for KSDs based on Stein's method, demonstrate that commonly used KSDs fail to detect non-convergence even for Gaussian targets, and show that kernels with slowly decaying tails provably determine convergence for a large class of target distributions. The resulting convergence-determining KSDs are suitable for comparing biased, exact, and deterministic sample sequences and simpler to compute and parallelize than alternative Stein discrepancies. We use our tools to compare biased samplers, select sampler hyperparameters, and improve upon existing KSD approaches to one-sample hypothesis testing and sample quality improvement.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/home/artem/Zotero/storage/J99AUBVP/Gorham and Mackey - 2017 - Measuring Sample Quality with Kernels.pdf}
}

@article{barbourSteinMethodPoisson1988,
  title = {Stein's {{Method}} and {{Poisson Process Convergence}}},
  author = {Barbour, A. D.},
  year = {1988},
  journal = {Journal of Applied Probability},
  volume = {25},
  eprint = {3214155},
  eprinttype = {jstor},
  pages = {175--184},
  issn = {0021-9002},
  doi = {10.2307/3214155},
  urldate = {2024-08-21},
  abstract = {Stein's method of obtaining rates of convergence, well known in normal and Poisson approximation, is considered here in the context of approximation by Poisson point processes, rather than their one-dimensional distributions. A general technique is sketched, whereby the basic ingredients necessary for the application of Stein's method may be derived, and this is applied to a simple problem in Poisson point process approximation.},
  file = {/home/artem/Zotero/storage/LEBSS5TQ/Barbour - 1988 - Stein's Method and Poisson Process Convergence.pdf}
}

@inproceedings{chenSteinPointMarkov2019,
  title = {Stein {{Point Markov Chain Monte Carlo}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Chen, Wilson Ye and Barp, Alessandro and Briol, Fran{\c c}ois-Xavier and Gorham, Jackson and Girolami, Mark and Mackey, Lester and Oates, {\relax Chris}. J.},
  year = {2019},
  pages = {1011--1021},
  publisher = {PLMR},
  address = {Long Beach, California},
  doi = {10.48550/ARXIV.1905.03673},
  urldate = {2024-08-21},
  abstract = {An important task in machine learning and statistics is the approximation of a probability measure by an empirical measure supported on a discrete point set. Stein Points are a class of algorithms for this task, which proceed by sequentially minimising a Stein discrepancy between the empirical measure and the target and, hence, require the solution of a non-convex optimisation problem to obtain each new point. This paper removes the need to solve this optimisation problem by, instead, selecting each new point based on a Markov chain sample path. This significantly reduces the computational cost of Stein Points and leads to a suite of algorithms that are straightforward to implement. The new algorithms are illustrated on a set of challenging Bayesian inference problems, and rigorous theoretical guarantees of consistency are established.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation (stat.CO),FOS: Computer and information sciences,FOS: Mathematics,Machine Learning (stat.ML),Methodology (stat.ME),Statistics Theory (math.ST)},
  file = {/home/artem/Zotero/storage/XVSMEIJE/1905.03673v2.pdf}
}

@article{detommasoSteinVariationalNewton2018,
  title = {A {{Stein}} Variational {{Newton}} Method},
  author = {Detommaso, Gianluca and Cui, Tiangang and Spantini, Alessio and Marzouk, Youssef and Scheichl, Robert},
  year = {2018},
  doi = {10.48550/ARXIV.1806.03085},
  urldate = {2024-08-22},
  abstract = {Stein variational gradient descent (SVGD) was recently proposed as a general purpose nonparametric variational inference algorithm [Liu \&amp; Wang, NIPS 2016]: it minimizes the Kullback-Leibler divergence between the target distribution and its approximation by implementing a form of functional gradient descent on a reproducing kernel Hilbert space. In this paper, we accelerate and generalize the SVGD algorithm by including second-order information, thereby approximating a Newton-like iteration in function space. We also show how second-order information can lead to more effective choices of kernel. We observe significant computational gains over the original SVGD algorithm in multiple test cases.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,FOS: Mathematics,Machine Learning (cs.LG),Machine Learning (stat.ML),Numerical Analysis (math.NA)},
  file = {/home/artem/Zotero/storage/XFGVFJ9J/Detommaso et al. - 2018 - A Stein variational Newton method.pdf}
}

@inproceedings{chenSteinPoints2018,
  title = {Stein {{Points}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Chen, Wilson Ye and Mackey, Lester and Gorham, Jackson and Briol, Fran{\c c}ois-Xavier and Oates, Chris J.},
  year = {2018},
  pages = {843--852},
  publisher = {PLMR},
  doi = {10.48550/ARXIV.1803.10161},
  urldate = {2024-08-22},
  abstract = {An important task in computational statistics and machine learning is to approximate a posterior distribution \$p(x)\$ with an empirical measure supported on a set of representative points \${\textbackslash}\{x\_i{\textbackslash}\}\_\{i=1\}{\textasciicircum}n\$. This paper focuses on methods where the selection of points is essentially deterministic, with an emphasis on achieving accurate approximation when \$n\$ is small. To this end, we present `Stein Points'. The idea is to exploit either a greedy or a conditional gradient method to iteratively minimise a kernel Stein discrepancy between the empirical measure and \$p(x)\$. Our empirical results demonstrate that Stein Points enable accurate approximation of the posterior at modest computational cost. In addition, theoretical results are provided to establish convergence of the method.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation (stat.CO),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/home/artem/Zotero/storage/C4N5E2M7/Chen et al. - 2018 - Stein Points.pdf}
}

@article{parkBayesianLasso2008,
  title = {The {{Bayesian Lasso}}},
  author = {Park, Trevor and Casella, George},
  year = {2008},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {103},
  number = {482},
  pages = {681--686},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214508000000337},
  urldate = {2024-08-23},
  langid = {english},
  file = {/home/artem/Zotero/storage/D5SR2JNG/Park and Casella - 2008 - The Bayesian Lasso.pdf}
}

@article{rockovaSpikeandSlabLASSO2018,
  title = {The {{Spike-and-Slab LASSO}}},
  author = {Ro{\v c}kov{\'a}, Veronika and George, Edward I.},
  year = {2018},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {113},
  number = {521},
  pages = {431--444},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2016.1260469},
  urldate = {2024-08-23},
  langid = {english},
  file = {/home/artem/Zotero/storage/KERCZPN4/Ročková and George - 2018 - The Spike-and-Slab LASSO.pdf}
}

@article{cockayneProbabilisticGradientsFast2021,
  title = {Probabilistic {{Gradients}} for {{Fast Calibration}} of {{Differential Equation Models}}},
  author = {Cockayne, Jonathan and Duncan, Andrew},
  year = {2021},
  month = jan,
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  volume = {9},
  number = {4},
  pages = {1643--1672},
  issn = {2166-2525},
  doi = {10.1137/20M1364424},
  urldate = {2024-08-23},
  langid = {english},
  file = {/home/artem/Zotero/storage/U2SF4K5S/Cockayne and Duncan - 2021 - Probabilistic Gradients for Fast Calibration of Di.pdf}
}

@book{robertMonteCarloStatistical2004,
  title = {Monte {{Carlo Statistical Methods}}},
  author = {Robert, Christian P. and Casella, George},
  year = {2004},
  series = {Springer {{Texts}} in {{Statistics}}},
  edition = {2nd},
  publisher = {Springer},
  address = {New York},
  doi = {10.1007/978-1-4757-4145-2},
  urldate = {2024-08-28},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-1-4419-1939-7 978-1-4757-4145-2},
  file = {/home/artem/Zotero/storage/V3US9VCF/Robert and Casella - 2004 - Monte Carlo Statistical Methods.pdf}
}
